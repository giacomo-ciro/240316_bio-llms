{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import time\n",
    "import copy\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import set_seed, AttrDict\n",
    "from myTorchtext import Vocab\n",
    "from preprocess import Preprocessor\n",
    "from tokenizer import tokenize_and_pad_batch, retrieve_tfs, random_mask_value\n",
    "from model import TransformerModel\n",
    "from loss import masked_mse_loss, masked_relative_error, criterion_neg_log_bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:x8697wty) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>train/mse</td><td>█▇▆▅▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/nzlp</td><td>█▆▄▃▃▃▃▂▂▃▂▂▃▃▂▂▂▃▃▃▂▂▂▃▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid/mre</td><td>▁</td></tr><tr><td>valid/mse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train/mse</td><td>165.19212</td></tr><tr><td>train/nzlp</td><td>0.00104</td></tr><tr><td>valid/mre</td><td>13912.42714</td></tr><tr><td>valid/mse</td><td>153.74775</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-galaxy-1</strong> at: <a href='https://wandb.ai/giacomo-ciro/BioFormer/runs/x8697wty' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer/runs/x8697wty</a><br/> View project at: <a href='https://wandb.ai/giacomo-ciro/BioFormer' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240801_163832-x8697wty\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:x8697wty). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\giaco\\Desktop\\240316_bio-llms\\wandb\\run-20240801_170032-p7p2fba2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giacomo-ciro/BioFormer/runs/p7p2fba2' target=\"_blank\">volcanic-hill-2</a></strong> to <a href='https://wandb.ai/giacomo-ciro/BioFormer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giacomo-ciro/BioFormer' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giacomo-ciro/BioFormer/runs/p7p2fba2' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer/runs/p7p2fba2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AttrDict({\n",
    "    \"run_name\": \"\",\n",
    "    \"dataset_name\": \"HYPOXIA_9K\",\n",
    "    \"seed\": 42,\n",
    "    \"ntokens\": 10000,\n",
    "    \"d_model\": 512,\n",
    "    \"nhead\": 8,\n",
    "    \"d_hid\": 2048,\n",
    "    \"nlayers\": 6,\n",
    "    \"n_bins\": 51,\n",
    "    \"n_hvg\": 100,\n",
    "    \"include_zero_gene\": False,\n",
    "    \"mask_single_value\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"amp\": True,\n",
    "    \"schedule_ratio\": 0.9,\n",
    "    \"GEPC\": False,   # If Gene Expression Prediction for Cell Modelling objective (MLM from <cls> only)\n",
    "    \"explicit_zero_prob\": True, # if modelling gene expression also as bern var\n",
    "    \"do_train\": True,\n",
    "    \"log_interval\": 100,\n",
    "    \"wandb\": True,\n",
    "})\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "if config.wandb:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(\n",
    "        project='BioFormer',\n",
    "        config = config,\n",
    "        name = config.run_name if config.run_name else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config.n_bins\n",
    "include_zero_gene = config.include_zero_gene\n",
    "n_hvg = config.n_hvg\n",
    "max_seq_len = n_hvg + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPOXIA_9K\n",
      "AnnData object with n_obs × n_vars = 9234 × 19046\n",
      "    obs: 'nCount_RNA', 'nFeature_RNA', 'SampleTags', 'percent.mt', 'HypoxicState', 'TimePoint', 'nCount_SCT', 'nFeature_SCT', 'S.Score', 'G2M.Score', 'Phase', 'seurat_clusters', 'SampleTagsShort', 'active_ident'\n",
      "    var: 'variable_gene', 'gene_name'\n",
      "    uns: 'active_ident_colors', 'seurat_clusters_colors'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    layers: 'raw_count'\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "dataset_name = config.dataset_name\n",
    "\n",
    "if dataset_name == 'BREAST_25K':\n",
    "    adata = sc.read_h5ad('./data/breast_25k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'BREAST_12K':\n",
    "    adata = sc.read_h5ad('./data/breast_12k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'DERMAL_100K':\n",
    "    adata = sc.read_h5ad('./data/dermal_100k.h5ad')\n",
    "    adata.var[\"gene_name\"] = adata.var.feature_name.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'HYPOXIA_9K':\n",
    "    adata = sc.read_h5ad('./data/scsHypoxiaTimeSub.h5ad')\n",
    "    adata.X = adata.layers['raw_count']\n",
    "    adata.var['gene_name'] = adata.var.index.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "print(dataset_name)\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering genes by counts ...\n",
      "Normalizing total counts ...\n",
      "Log1p transforming ...\n",
      "Subsetting highly variable genes ...\n",
      "No batch_key is provided, will use all cells for HVG selection.\n",
      "Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# Pre-process adata\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=config.n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab of size: 103 --> 100 genes, 3 special tokens ['<pad>', '<cls>', '<eoc>']\n"
     ]
    }
   ],
   "source": [
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].toarray()\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "train_data, valid_data = train_test_split(all_counts, test_size=0.1, shuffle=True)\n",
    "\n",
    "# Vocab\n",
    "stoi = {s:i for i, s in enumerate(genes + special_tokens)}\n",
    "itos = {i:s for i, s in enumerate(genes + special_tokens)}\n",
    "vocab = Vocab(stoi, itos)\n",
    "vocab.set_default_index(vocab[\"<pad>\"]) # index to return if token not found in vocab\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "print(f'Vocab of size: {len(vocab)} --> {len(genes)} genes, {len(special_tokens)} special tokens {special_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 8310\n",
      "Valid samples: 924\n",
      "Input length: 68\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "print(f\"Train samples: {tokenized_train['genes'].shape[0]}\")\n",
    "print(f\"Valid samples: {tokenized_valid['genes'].shape[0]}\")\n",
    "print(f\"Input length: {tokenized_valid['genes'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import TFs Lookup Table\n",
    "# if config.use_condition_labels:\n",
    "#     tf = pd.read_csv(r'./data/transcriptional_interactions.csv',\n",
    "#                             index_col=0, \n",
    "#                             low_memory=False,\n",
    "#                             dtype={'source_genesymbol': str, 'target_genesymbol':str})[['source_genesymbol', 'target_genesymbol', 'is_stimulation', 'is_inhibition']].rename(columns={'source_genesymbol': 'source', 'target_genesymbol':'target'})\n",
    "#     tf = tf.drop_duplicates()\n",
    "#     tf = tf[~(tf.is_inhibition  == tf.is_stimulation)]    # drop rows where both are 1s or 0s\n",
    "\n",
    "#     source_in_vocab = tf.source.isin(vocab.get_stoi().keys())\n",
    "#     target_in_vocab = tf.target.isin(vocab.get_stoi().keys())\n",
    "#     both_in_vocab = source_in_vocab * target_in_vocab\n",
    "\n",
    "#     print('Unique Sources in GeneVocab:',\n",
    "#             tf[source_in_vocab].source.unique().shape[0])\n",
    "#     print('Unique Targets in GeneVocab:',\n",
    "#             tf[target_in_vocab].target.unique().shape[0])\n",
    "\n",
    "#     tf = tf[both_in_vocab]\n",
    "\n",
    "#     print('Unique Pairs in GeneVocab:',\n",
    "#             tf[~tf[['source', 'target']].duplicated()].shape[0])\n",
    "\n",
    "#     print('Stimulation Interactions:',\n",
    "#             tf.is_stimulation.sum())\n",
    "\n",
    "#     print('Inhibition Interactions:',\n",
    "#             tf.is_inhibition.sum())\n",
    "\n",
    "#     tf.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(use_condition_labels = False):\n",
    "    \n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "        mask_single_value = config.mask_single_value\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "        mask_single_value = config.mask_single_value\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "    }\n",
    "\n",
    "    if use_condition_labels:\n",
    "        train_data_pt['conditions'] = retrieve_tfs(\n",
    "            input_gene_ids_train,\n",
    "            input_values_train,     # masked\n",
    "            tf = tf                                  \n",
    "        )\n",
    "        valid_data_pt['conditions'] = retrieve_tfs(\n",
    "            input_gene_ids_valid,\n",
    "            input_values_valid,      # masked\n",
    "            tf = tf                                  \n",
    "        )\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: cpu | d_model: 2048 | nhead: 8 | nlayers: 6 | tot. params: 12M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ntoken = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntoken=ntoken,\n",
    "    d_model=config.d_model,\n",
    "    nhead=config.nhead,\n",
    "    d_hid=config.d_model,\n",
    "    nlayers=config.nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "\n",
    "    # use_condition_labels=config.use_condition_labels,\n",
    "    # num_condition_labels= 3 if config.use_condition_labels else None, # (-1, 0, 1)\n",
    ")\n",
    "# if config.load_model is not None:\n",
    "#     load_pretrained(model, torch.load(model_file), verbose=False)\n",
    "#     print('Pre-trained model successfully loaded')\n",
    "    \n",
    "model.to(device)\n",
    "# wandb.watch(model)\n",
    "\n",
    "print(f'''\n",
    "device: {device} | d_model: {config.d_hid} | nhead: {config.nhead} | nlayers: {config.nlayers} | tot. params: {sum(p.numel() for p in model.parameters())/1e6:.0f}M\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_error = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        # input_conditions = batch_data[\"conditions\"].to(device) if 'conditions' in batch_data.keys() else None\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                # src_key_padding_mask=src_key_padding_mask,\n",
    "                # batch_labels=batch_labels if DSBN else None,\n",
    "                # conditions = input_conditions,\n",
    "                # MVC=config.GEPC,\n",
    "                # ECS=config.ecs_thres > 0,\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = criterion(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            \n",
    "            if config.explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mlm_zero_probs\"], target_values, masked_positions)\n",
    "                loss += loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            \n",
    "            if config.GEPC:\n",
    "                loss_gepc = criterion(output_dict[\"mvc_output\"], target_values, masked_positions)\n",
    "                loss += loss_gepc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "            \n",
    "            if config.GEPC and config.explicit_zero_prob:\n",
    "                loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mvc_zero_probs\"], target_values, masked_positions)\n",
    "                loss = loss + loss_gepc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()})\n",
    "            \n",
    "            # if config.ecs_thres > 0:\n",
    "            #     loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "            #     loss = loss + loss_ecs\n",
    "            #     metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            # loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "            # loss = loss + config.dab_weight * loss_dab\n",
    "            # metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        # with warnings.catch_warnings(record=True) as w:\n",
    "        #     warnings.filterwarnings(\"always\")\n",
    "        #     torch.nn.utils.clip_grad_norm_(\n",
    "        #         model.parameters(),\n",
    "        #         1.0,\n",
    "        #         error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "        #     )\n",
    "        #     if len(w) > 0:\n",
    "        #         print(\n",
    "        #             f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "        #             f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "        #             \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "        #         )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item()\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0\n",
    "        total_error += mre.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\"\n",
    "                + (f\"gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\")\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_wandb_metrcis():\n",
    "#     wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "#     wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "#     wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "#     wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "#     wandb.define_metric(\"test/avg_bio\", summary=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            # input_conditions = batch_data[\"conditions\"].to(device) if 'conditions' in batch_data.keys() else None\n",
    "\n",
    "            # src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    # src_key_padding_mask=src_key_padding_mask,\n",
    "                    # batch_labels=batch_labels if DSBN else None,\n",
    "                    # conditions = input_conditions,\n",
    "                )\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "                # loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_error += masked_relative_error(\n",
    "                output_values, target_values, masked_positions\n",
    "            ).item() * len(input_gene_ids)\n",
    "            # total_dab += loss_dab.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_error / total_num,\n",
    "            # \"valid/dab\": total_dab / total_num,\n",
    "            # \"valid/sum_mse_dab\": (total_loss + config.dab_weight * total_dab)\n",
    "            # / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 100/260 batches | lr 0.0001 | ms/batch 2718.67 | loss 351.74 | mse 351.74 | mre 446295.37 |\n",
      "| epoch   1 | 200/260 batches | lr 0.0001 | ms/batch 1879.34 | loss 185.04 | mse 185.04 | mre 377947.74 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 586.96s | valid loss/mse 153.7478 | mre 13912.4271\n",
      "-----------------------------------------------------------------------------------------\n",
      "Best model with score 153.7478\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "# define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_data_pt, valid_data_pt = prepare_data()\n",
    "    \n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "\n",
    "    val_loss, val_mre = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print(\"-\" * 89)\n",
    "    print(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "    )\n",
    "    print(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        print(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    # if epoch % config.save_eval_interval == 0 or epoch == config.epochs:\n",
    "    #     print(f\"Saving model to {save_dir}\")\n",
    "    #     torch.save(best_model.state_dict(), save_dir / f\"model_e{best_model_epoch}.pt\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-hill-2</strong> at: <a href='https://wandb.ai/giacomo-ciro/BioFormer/runs/p7p2fba2' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer/runs/p7p2fba2</a><br/> View project at: <a href='https://wandb.ai/giacomo-ciro/BioFormer' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240801_170032-p7p2fba2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if config.wandb:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def eval_testdata(\n",
    "#     model: nn.Module,\n",
    "#     adata_t: AnnData,\n",
    "#     include_types: List[str] = [\"cls\"],\n",
    "# ) -> Optional[Dict]:\n",
    "#     \"\"\"evaluate the model on test dataset of adata_t\"\"\"\n",
    "#     model.eval()\n",
    "\n",
    "#     # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "#     adata_t = adata_t.copy()\n",
    "\n",
    "#     all_counts = (\n",
    "#         adata_t.layers[input_layer_key].A\n",
    "#         if issparse(adata_t.layers[input_layer_key])\n",
    "#         else adata_t.layers[input_layer_key]\n",
    "#     )\n",
    "\n",
    "#     celltypes_labels = adata_t.obs[\"celltype\"].tolist()\n",
    "#     celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "#     batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "#     batch_ids = np.array(batch_ids)\n",
    "\n",
    "#     # Evaluate cls cell embeddings\n",
    "#     if \"cls\" in include_types:\n",
    "#         logger.info(\"Evaluating cls cell embeddings\")\n",
    "#         tokenized_all = tokenize_and_pad_batch(\n",
    "#             all_counts,\n",
    "#             gene_ids,\n",
    "#             max_len=max_seq_len,\n",
    "#             vocab=vocab,\n",
    "#             pad_token=pad_token,\n",
    "#             pad_value=pad_value,\n",
    "#             append_cls=True,  # append <cls> token at the beginning\n",
    "#             include_zero_gene=True,\n",
    "#         )\n",
    "#         all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "#         src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "#         with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "#             cell_embeddings = model.encode_batch(\n",
    "#                 all_gene_ids,\n",
    "#                 all_values.float(),\n",
    "#                 src_key_padding_mask=src_key_padding_mask,\n",
    "#                 batch_size=config.batch_size,\n",
    "#                 batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "#                 time_step=0,\n",
    "#                 return_np=True,\n",
    "#             )\n",
    "#         cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "#             cell_embeddings, axis=1, keepdims=True\n",
    "#         )\n",
    "\n",
    "#         adata_t.obsm[\"X_scGPT\"] = cell_embeddings\n",
    "\n",
    "#         results = {}\n",
    "#         try:\n",
    "#             results = eval_scib_metrics(adata_t)\n",
    "#         except Exception as e:\n",
    "#             traceback.print_exc()\n",
    "#             logger.error(e)\n",
    "\n",
    "#         sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "#         sc.tl.umap(adata_t, min_dist=0.3)\n",
    "#         fig = sc.pl.umap(\n",
    "#             adata_t,\n",
    "#             color=[\"str_batch\"],\n",
    "#             title=[f\"batch, avg_bio = {results.get('avg_bio', 0.0):.4f}\"],\n",
    "#             frameon=False,\n",
    "#             return_fig=True,\n",
    "#             show=False,\n",
    "#         )\n",
    "\n",
    "#         results[\"batch_umap\"] = fig\n",
    "\n",
    "#         sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "#         sc.tl.umap(adata_t, min_dist=0.3)\n",
    "#         fig = sc.pl.umap(\n",
    "#             adata_t,\n",
    "#             color=[\"celltype\"],\n",
    "#             title=[\n",
    "#                 f\"celltype, avg_bio = {results.get('avg_bio', 0.0):.4f}\",\n",
    "#             ],\n",
    "#             frameon=False,\n",
    "#             return_fig=True,\n",
    "#             show=False,\n",
    "#         )\n",
    "\n",
    "#         results[\"celltype_umap\"] = fig\n",
    "\n",
    "#     if len(include_types) == 1:\n",
    "#         return results\n",
    "\n",
    "\n",
    "\n",
    "# # Investigate one batch\n",
    "# batch_data = next(iter(train_loader))\n",
    "# input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "# input_values = batch_data[\"values\"].to(device)\n",
    "# target_values = batch_data[\"target_values\"].to(device)\n",
    "# input_conditions = batch_data[\"conditions\"].to(device) if 'conditions' in batch_data.keys() else None\n",
    "# src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "# with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "#     output_dict = model(\n",
    "#         input_gene_ids,\n",
    "#         input_values,\n",
    "#         src_key_padding_mask=src_key_padding_mask,\n",
    "#         # batch_labels=batch_labels if DSBN else None,\n",
    "#         # conditions = input_conditions,\n",
    "#         # MVC=config.GEPC,\n",
    "#         # ECS=config.ecs_thres > 0,\n",
    "#     )\n",
    "\n",
    "# masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "# print(output_dict['mlm_output'][masked_positions])\n",
    "# print(target_values[masked_positions])\n",
    "\n",
    "# save the best model\n",
    "# torch.save(best_model.state_dict(), save_dir / \"best_model.pt\")\n",
    "\n",
    "# artifact = wandb.Artifact(f\"best_model\", type=\"model\")\n",
    "# glob_str = os.path.join(save_dir, \"best_model.pt\")\n",
    "# artifact.add_file(glob_str)\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# run.finish()\n",
    "# wandb.finish()\n",
    "# gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
