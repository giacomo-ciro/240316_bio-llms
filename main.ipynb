{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scGPT\n",
    "Custom implemention of the scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import time\n",
    "import copy\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import set_seed, AttrDict\n",
    "from myTorchtext import Vocab\n",
    "from preprocess import Preprocessor\n",
    "from tokenizer import tokenize_and_pad_batch, retrieve_tfs, random_mask_value\n",
    "from model import TransformerModel\n",
    "from loss import masked_mse_loss, masked_relative_error, criterion_neg_log_bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: brightmanx (giacomo-ciro). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\giaco\\Desktop\\240316_bio-llms\\wandb\\run-20240902_162226-ahbxhy4k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giacomo-ciro/BioFormer/runs/ahbxhy4k' target=\"_blank\">denim-water-4</a></strong> to <a href='https://wandb.ai/giacomo-ciro/BioFormer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giacomo-ciro/BioFormer' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giacomo-ciro/BioFormer/runs/ahbxhy4k' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer/runs/ahbxhy4k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AttrDict({\n",
    "    \"run_name\": \"\",\n",
    "    \"dataset_name\": \"HYPOXIA_9K\",\n",
    "    \"seed\": None,\n",
    "    \"ntokens\": 10000,\n",
    "    \"d_model\": 512,\n",
    "    \"nhead\": 8,\n",
    "    \"d_hid\": 2048,\n",
    "    \"nlayers\": 6,\n",
    "    \"n_bins\": 51,\n",
    "    \"n_hvg\": 100,\n",
    "    \"include_zero_gene\": False,\n",
    "    \"mask_single_value\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"amp\": True,\n",
    "    \"schedule_ratio\": 0.9,\n",
    "    \"GEPC\": False,   # If Gene Expression Prediction for Cell Modelling objective (MLM from <cls> only) TODO in model.py\n",
    "    \"explicit_zero_prob\": True, # if modelling gene expression also as bern var\n",
    "    \"do_train\": True,\n",
    "    \"log_interval\": 100,\n",
    "    \"wandb\": True,\n",
    "})\n",
    "\n",
    "if config.seed:\n",
    "    set_seed(config.seed)\n",
    "\n",
    "if config.wandb:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(\n",
    "        project='BioFormer',\n",
    "        config = config,\n",
    "        name = config.run_name if config.run_name else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config.n_bins\n",
    "include_zero_gene = config.include_zero_gene\n",
    "n_hvg = config.n_hvg\n",
    "max_seq_len = n_hvg + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPOXIA_9K\n",
      "AnnData object with n_obs × n_vars = 9234 × 19046\n",
      "    obs: 'nCount_RNA', 'nFeature_RNA', 'SampleTags', 'percent.mt', 'HypoxicState', 'TimePoint', 'nCount_SCT', 'nFeature_SCT', 'S.Score', 'G2M.Score', 'Phase', 'seurat_clusters', 'SampleTagsShort', 'active_ident'\n",
      "    var: 'variable_gene', 'gene_name'\n",
      "    uns: 'active_ident_colors', 'seurat_clusters_colors'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    layers: 'raw_count'\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "dataset_name = config.dataset_name\n",
    "\n",
    "if dataset_name == 'BREAST_25K':\n",
    "    adata = sc.read_h5ad('./data/breast_25k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'BREAST_12K':\n",
    "    adata = sc.read_h5ad('./data/breast_12k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'DERMAL_100K':\n",
    "    adata = sc.read_h5ad('./data/dermal_100k.h5ad')\n",
    "    adata.var[\"gene_name\"] = adata.var.feature_name.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'HYPOXIA_9K':\n",
    "    adata = sc.read_h5ad('./data/scsHypoxiaTimeSub.h5ad')\n",
    "    adata.X = adata.layers['raw_count']\n",
    "    adata.var['gene_name'] = adata.var.index.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "print(dataset_name)\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering genes by counts ...\n",
      "Normalizing total counts ...\n",
      "Log1p transforming ...\n",
      "Subsetting highly variable genes ...\n",
      "No batch_key is provided, will use all cells for HVG selection.\n",
      "Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# Pre-process adata\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=config.n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab of size: 103 --> 100 genes, 3 special tokens ['<pad>', '<cls>', '<eoc>']\n"
     ]
    }
   ],
   "source": [
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].toarray()\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "train_data, valid_data = train_test_split(all_counts, test_size=0.1, shuffle=True)\n",
    "\n",
    "# Vocab\n",
    "stoi = {s:i for i, s in enumerate(genes + special_tokens)}\n",
    "itos = {i:s for i, s in enumerate(genes + special_tokens)}\n",
    "vocab = Vocab(stoi, itos)\n",
    "vocab.set_default_index(vocab[\"<pad>\"]) # index to return if token not found in vocab\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "print(f'Vocab of size: {len(vocab)} --> {len(genes)} genes, {len(special_tokens)} special tokens {special_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 8310\n",
      "Valid samples: 924\n",
      "Input length: 68\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "print(f\"Train samples: {tokenized_train['genes'].shape[0]}\")\n",
    "print(f\"Valid samples: {tokenized_valid['genes'].shape[0]}\")\n",
    "print(f\"Input length: {tokenized_valid['genes'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(use_condition_labels = False):\n",
    "    \n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "        mask_single_value = config.mask_single_value\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "        mask_single_value = config.mask_single_value\n",
    "    )\n",
    "\n",
    "    print(f\"random masking at epoch {epoch}, ratio of masked values in train: {(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\")\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "    }\n",
    "\n",
    "    # if use_condition_labels:\n",
    "    #     train_data_pt['conditions'] = retrieve_tfs(\n",
    "    #         input_gene_ids_train,\n",
    "    #         input_values_train,     # masked\n",
    "    #         tf = tf                                  \n",
    "    #     )\n",
    "    #     valid_data_pt['conditions'] = retrieve_tfs(\n",
    "    #         input_gene_ids_valid,\n",
    "    #         input_values_valid,      # masked\n",
    "    #         tf = tf                                  \n",
    "    #     )\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: cpu | d_model: 2048 | nhead: 8 | nlayers: 6 | tot. params: 12M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ntoken = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntoken=ntoken,\n",
    "    d_model=config.d_model,\n",
    "    nhead=config.nhead,\n",
    "    d_hid=config.d_model,\n",
    "    nlayers=config.nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    ") \n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f'''\n",
    "device: {device} | d_model: {config.d_hid} | nhead: {config.nhead} | nlayers: {config.nlayers} | tot. params: {sum(p.numel() for p in model.parameters())/1e6:.0f}M\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_mre = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "\n",
    "        # ---------- FORWARD PASS -------------------\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "\n",
    "            output_dict = model(input_gene_ids, input_values)\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = criterion(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "            \n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            \n",
    "            if config.explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mlm_zero_probs\"], target_values, masked_positions)\n",
    "                loss += loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            \n",
    "            if config.GEPC:\n",
    "                loss_gepc = criterion(output_dict[\"mvc_output\"], target_values, masked_positions)\n",
    "                loss += loss_gepc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "            \n",
    "            if config.GEPC and config.explicit_zero_prob:\n",
    "                loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mvc_zero_probs\"], target_values, masked_positions)\n",
    "                loss = loss + loss_gepc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()})\n",
    "\n",
    "        # ---------- BACKWARD PASS ------------------\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()   # training via the aggregated loss\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # -------------------------------------------\n",
    "        \n",
    "        if config.wandb:\n",
    "            wandb.log(metrics_to_log)\n",
    "\n",
    "        # Compute MRE for validation\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "\n",
    "        total_loss += loss.item()                               # sum of all losses\n",
    "        total_mse += loss_mse.item()                            # MSE alone\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0  # MSE from GEPC alone\n",
    "        total_mre += mre.item()                                 # MRE alone\n",
    "        \n",
    "        # Avg of loss across all log_interval batches (i.e., log_interval = 10, avg loss every 10 batches)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_mre = total_mre / log_interval\n",
    "            \n",
    "            print(f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | train/loss {cur_loss:5.2f} | train/mse {cur_mse:5.2f} |\" + (f\"train/gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\") + f\"train/mre {cur_mre:5.2f} |\" )\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_mre = 0\n",
    "            start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_wandb_metrics():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mre = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(input_gene_ids, input_values)\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_mre += masked_relative_error(output_values, target_values, masked_positions).item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    if config.wandb:\n",
    "        wandb.log({ \n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_mre / total_num,\n",
    "            \"epoch\": epoch\n",
    "            })\n",
    "\n",
    "    return total_loss / total_num, total_mre / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 100/260 batches | lr 0.0001 | ms/batch 1930.85 | train/loss 351.74 | train/mse 351.74 | train/mre 446295.37 |\n",
      "| epoch   1 | 200/260 batches | lr 0.0001 | ms/batch 2243.27 | train/loss 185.04 | train/mse 185.04 | train/mre 377947.74 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | runtime: 544.98s | valid/mse 153.7478 | valid/mre 13912.4271\n",
      "-----------------------------------------------------------------------------------------\n",
      "Best model with score 153.7478\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrics()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_data_pt, valid_data_pt = prepare_data()\n",
    "    \n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # TRAINING      --> over all batches in the train_loader\n",
    "    if config.do_train:\n",
    "        train(model, loader=train_loader)\n",
    "\n",
    "    # VALIDATION    --> avg loss across all batches in valid_loader\n",
    "    val_loss, val_mre = evaluate(model, loader=valid_loader)\n",
    "    \n",
    "    \n",
    "    # Some epoch-related stats\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print(\"-\" * 89)\n",
    "    print(f\"| end of epoch {epoch:3d} | runtime: {elapsed:5.2f}s | valid/mse {val_loss:5.4f} | valid/mre {val_mre:5.4f}\")\n",
    "    print(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        print(f\"Best model with valid/mse {best_val_loss:5.4f}\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>train/mse</td><td>█▇▆▅▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/nzlp</td><td>█▆▄▃▃▃▃▂▂▃▂▂▃▃▂▂▂▃▃▃▂▂▂▃▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid/mre</td><td>▁</td></tr><tr><td>valid/mse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train/mse</td><td>165.19212</td></tr><tr><td>train/nzlp</td><td>0.00104</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-water-4</strong> at: <a href='https://wandb.ai/giacomo-ciro/BioFormer/runs/ahbxhy4k' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer/runs/ahbxhy4k</a><br/> View project at: <a href='https://wandb.ai/giacomo-ciro/BioFormer' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240902_162226-ahbxhy4k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if config.wandb:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioFormer\n",
    "Adaptation of AF2 modules to work with RNA-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0929,  0.0206,  0.0192,  ..., -0.0779, -0.0588,  0.0711],\n",
       "         [-0.0088,  0.0091,  0.0384,  ..., -0.0276, -0.0663,  0.0664],\n",
       "         [-0.0630, -0.0031,  0.0276,  ..., -0.0101, -0.0579,  0.0569],\n",
       "         ...,\n",
       "         [-0.0403,  0.0068,  0.0426,  ..., -0.0284, -0.0402,  0.0621],\n",
       "         [-0.0754,  0.0102,  0.0090,  ...,  0.0038, -0.0409,  0.1010],\n",
       "         [-0.0073, -0.0207,  0.0712,  ..., -0.0377, -0.0642,  0.0093]],\n",
       "\n",
       "        [[ 0.0042,  0.0051, -0.0396,  ..., -0.0015, -0.0125,  0.0598],\n",
       "         [ 0.0407,  0.0388, -0.0429,  ...,  0.0148, -0.0269,  0.0098],\n",
       "         [ 0.0487, -0.0179, -0.0329,  ...,  0.0232, -0.0640,  0.0544],\n",
       "         ...,\n",
       "         [ 0.0351,  0.0055,  0.0059,  ...,  0.0316, -0.0342,  0.0403],\n",
       "         [ 0.0063,  0.0361, -0.0461,  ...,  0.0301, -0.0101, -0.0084],\n",
       "         [ 0.0490,  0.0098, -0.0145,  ..., -0.0182, -0.0377,  0.0050]],\n",
       "\n",
       "        [[ 0.1091,  0.0757,  0.0532,  ..., -0.0823, -0.1006, -0.1028],\n",
       "         [ 0.0654,  0.0863,  0.0859,  ..., -0.0665, -0.0980, -0.0227],\n",
       "         [ 0.0839,  0.1084,  0.1393,  ..., -0.0402, -0.1093, -0.0627],\n",
       "         ...,\n",
       "         [ 0.1050,  0.1288,  0.1188,  ..., -0.0644, -0.0944, -0.1261],\n",
       "         [ 0.1005,  0.1529,  0.0943,  ..., -0.0414, -0.0796, -0.0972],\n",
       "         [ 0.0906,  0.0354,  0.1111,  ..., -0.0171, -0.1004, -0.0813]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0201,  0.0106,  0.0226,  ...,  0.0249, -0.0578,  0.0028],\n",
       "         [ 0.0077,  0.0289, -0.0092,  ...,  0.0165,  0.0178,  0.0257],\n",
       "         [-0.0185, -0.0111,  0.0222,  ..., -0.0156, -0.0485, -0.0079],\n",
       "         ...,\n",
       "         [ 0.0305,  0.0571, -0.0193,  ...,  0.0071,  0.0635, -0.0036],\n",
       "         [ 0.0362, -0.0114,  0.0154,  ...,  0.0111, -0.0508,  0.0058],\n",
       "         [ 0.0470, -0.0085, -0.0014,  ...,  0.0323,  0.0300,  0.0321]],\n",
       "\n",
       "        [[-0.0069,  0.0909, -0.0468,  ..., -0.0535, -0.0734,  0.0450],\n",
       "         [ 0.0329,  0.0631, -0.0435,  ...,  0.0013, -0.0548, -0.0092],\n",
       "         [-0.0048,  0.0849, -0.0621,  ..., -0.0337, -0.0988,  0.0214],\n",
       "         ...,\n",
       "         [ 0.0376,  0.0177, -0.0332,  ...,  0.0027, -0.0052,  0.0575],\n",
       "         [ 0.0547,  0.1124, -0.0253,  ...,  0.0160, -0.0385,  0.0303],\n",
       "         [ 0.0103,  0.1548, -0.0250,  ...,  0.0107, -0.1012,  0.0215]],\n",
       "\n",
       "        [[ 0.0005, -0.0218, -0.0304,  ...,  0.0185, -0.0565,  0.0440],\n",
       "         [ 0.0834, -0.0540,  0.0050,  ...,  0.0469,  0.0008,  0.0386],\n",
       "         [ 0.0356, -0.0231,  0.0239,  ..., -0.0018, -0.1549,  0.0653],\n",
       "         ...,\n",
       "         [ 0.0303,  0.0048, -0.0012,  ...,  0.0082, -0.0661,  0.1083],\n",
       "         [ 0.0581,  0.0084,  0.0085,  ...,  0.0119,  0.0328, -0.0155],\n",
       "         [-0.0213, -0.0352, -0.0308,  ...,  0.0388, -0.0462,  0.0361]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from bioformer import RowAttentionWithPairBias\n",
    "\n",
    "B, r = 64, 18\n",
    "\n",
    "c_in = 8\n",
    "c_hidden = 64\n",
    "no_heads = 4\n",
    "\n",
    "attn = RowAttentionWithPairBias(\n",
    "    c_in=c_in, \n",
    "    c_hidden=c_hidden,\n",
    "    no_heads=no_heads,\n",
    "    pair_bias=True,\n",
    "    c_z=c_in,\n",
    "    gating=True\n",
    ")\n",
    "\n",
    "rna = torch.rand((B, r, c_in ))\n",
    "tf = torch.rand((B, r, r, c_in))\n",
    "\n",
    "attn(rna, tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
