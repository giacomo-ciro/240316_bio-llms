{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scGPT\n",
    "Custom implemention of the scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import time\n",
    "import copy\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import set_seed, AttrDict\n",
    "from myTorchtext import Vocab\n",
    "from preprocess import Preprocessor\n",
    "from tokenizer import tokenize_and_pad_batch, retrieve_tfs, random_mask_value\n",
    "# from model import TransformerModel\n",
    "from model_bioformer import TransformerModel\n",
    "from loss import masked_mse_loss, masked_relative_error, criterion_neg_log_bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AttrDict({\n",
    "    \"run_name\": \"\",\n",
    "    \"dataset_name\": \"HYPOXIA_9K\",\n",
    "    \"seed\": None,\n",
    "    \"ntokens\": 10000,\n",
    "    \"d_model\": 512,\n",
    "    \"nhead\": 4,\n",
    "    \"d_hid\": 128,\n",
    "    \"nlayers\": 6,\n",
    "    \"n_bins\": 51,\n",
    "    \"n_hvg\": 100,\n",
    "    \"include_zero_gene\": False,\n",
    "    \"mask_single_value\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"amp\": True,\n",
    "    \"schedule_ratio\": 0.9,\n",
    "    \"GEPC\": False,   # If Gene Expression Prediction for Cell Modelling objective (MLM from <cls> only) TODO in model.py\n",
    "    \"explicit_zero_prob\": True, # if modelling gene expression also as bern var\n",
    "    \"do_train\": True,\n",
    "    \"log_interval\": 100,\n",
    "    \"wandb\": False,\n",
    "})\n",
    "\n",
    "if config.seed:\n",
    "    set_seed(config.seed)\n",
    "\n",
    "if config.wandb:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(\n",
    "        project='BioFormer',\n",
    "        config = config,\n",
    "        name = config.run_name if config.run_name else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config.n_bins\n",
    "include_zero_gene = config.include_zero_gene\n",
    "n_hvg = config.n_hvg\n",
    "max_seq_len = n_hvg + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPOXIA_9K\n",
      "AnnData object with n_obs × n_vars = 9234 × 19046\n",
      "    obs: 'nCount_RNA', 'nFeature_RNA', 'SampleTags', 'percent.mt', 'HypoxicState', 'TimePoint', 'nCount_SCT', 'nFeature_SCT', 'S.Score', 'G2M.Score', 'Phase', 'seurat_clusters', 'SampleTagsShort', 'active_ident'\n",
      "    var: 'variable_gene', 'gene_name'\n",
      "    uns: 'active_ident_colors', 'seurat_clusters_colors'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    layers: 'raw_count'\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "dataset_name = config.dataset_name\n",
    "\n",
    "if dataset_name == 'BREAST_25K':\n",
    "    adata = sc.read_h5ad('./data/breast_25k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'BREAST_12K':\n",
    "    adata = sc.read_h5ad('./data/breast_12k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'DERMAL_100K':\n",
    "    adata = sc.read_h5ad('./data/dermal_100k.h5ad')\n",
    "    adata.var[\"gene_name\"] = adata.var.feature_name.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'HYPOXIA_9K':\n",
    "    adata = sc.read_h5ad('./data/scsHypoxiaTimeSub.h5ad')\n",
    "    adata.X = adata.layers['raw_count']\n",
    "    adata.var['gene_name'] = adata.var.index.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "print(dataset_name)\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering genes by counts ...\n",
      "Normalizing total counts ...\n",
      "Log1p transforming ...\n",
      "Subsetting highly variable genes ...\n",
      "No batch_key is provided, will use all cells for HVG selection.\n",
      "Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# Pre-process adata\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=config.n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab of size: 103 --> 100 genes, 3 special tokens ['<pad>', '<cls>', '<eoc>']\n"
     ]
    }
   ],
   "source": [
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].toarray()\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "train_data, valid_data = train_test_split(all_counts, test_size=0.1, shuffle=True)\n",
    "\n",
    "# Vocab\n",
    "stoi = {s:i for i, s in enumerate(genes + special_tokens)}\n",
    "itos = {i:s for i, s in enumerate(genes + special_tokens)}\n",
    "vocab = Vocab(stoi, itos)\n",
    "vocab.set_default_index(vocab[\"<pad>\"]) # index to return if token not found in vocab\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "print(f'Vocab of size: {len(vocab)} --> {len(genes)} genes, {len(special_tokens)} special tokens {special_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 8310\n",
      "Valid samples: 924\n",
      "Input length: 69\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "print(f\"Train samples: {tokenized_train['genes'].shape[0]}\")\n",
    "print(f\"Valid samples: {tokenized_valid['genes'].shape[0]}\")\n",
    "print(f\"Input length: {tokenized_valid['genes'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(use_condition_labels = False):\n",
    "    \n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "        mask_single_value = config.mask_single_value\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "        mask_single_value = config.mask_single_value\n",
    "    )\n",
    "\n",
    "    print(f\"random masking at epoch {epoch}, ratio of masked values in train: {(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\")\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "    }\n",
    "\n",
    "    # if use_condition_labels:\n",
    "    #     train_data_pt['conditions'] = retrieve_tfs(\n",
    "    #         input_gene_ids_train,\n",
    "    #         input_values_train,     # masked\n",
    "    #         tf = tf                                  \n",
    "    #     )\n",
    "    #     valid_data_pt['conditions'] = retrieve_tfs(\n",
    "    #         input_gene_ids_valid,\n",
    "    #         input_values_valid,      # masked\n",
    "    #         tf = tf                                  \n",
    "    #     )\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: cpu | d_model: 128 | nhead: 4 | nlayers: 6 | tot. params: 855M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ntoken = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntoken=ntoken,\n",
    "    d_model=config.d_model,\n",
    "    nhead=config.nhead,\n",
    "    d_hid=config.d_model,\n",
    "    nlayers=config.nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    ") \n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f'''\n",
    "device: {device} | d_model: {config.d_hid} | nhead: {config.nhead} | nlayers: {config.nlayers} | tot. params: {sum(p.numel() for p in model.parameters())/1e6:.0f}M\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_mre = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "\n",
    "        # ---------- FORWARD PASS -------------------\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "\n",
    "            output_dict = model(input_gene_ids, input_values)\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = criterion(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "            \n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            \n",
    "            if config.explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mlm_zero_probs\"], target_values, masked_positions)\n",
    "                loss += loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            \n",
    "            if config.GEPC:\n",
    "                loss_gepc = criterion(output_dict[\"mvc_output\"], target_values, masked_positions)\n",
    "                loss += loss_gepc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "            \n",
    "            if config.GEPC and config.explicit_zero_prob:\n",
    "                loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mvc_zero_probs\"], target_values, masked_positions)\n",
    "                loss = loss + loss_gepc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()})\n",
    "\n",
    "        # ---------- BACKWARD PASS ------------------\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()   # training via the aggregated loss\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # -------------------------------------------\n",
    "        \n",
    "        if config.wandb:\n",
    "            wandb.log(metrics_to_log)\n",
    "\n",
    "        # Compute MRE for validation\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "\n",
    "        total_loss += loss.item()                               # sum of all losses\n",
    "        total_mse += loss_mse.item()                            # MSE alone\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0  # MSE from GEPC alone\n",
    "        total_mre += mre.item()                                 # MRE alone\n",
    "        \n",
    "        # Avg of loss across all log_interval batches (i.e., log_interval = 10, avg loss every 10 batches)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_mre = total_mre / log_interval\n",
    "            \n",
    "            print(f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | train/loss {cur_loss:5.2f} | train/mse {cur_mse:5.2f} |\" + (f\"train/gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\") + f\"train/mre {cur_mre:5.2f} |\" )\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_mre = 0\n",
    "            start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_wandb_metrics():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mre = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(input_gene_ids, input_values)\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_mre += masked_relative_error(output_values, target_values, masked_positions).item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    if config.wandb:\n",
    "        wandb.log({ \n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_mre / total_num,\n",
    "            \"epoch\": epoch\n",
    "            })\n",
    "\n",
    "    return total_loss / total_num, total_mre / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch 1, ratio of masked values in train: 0.1382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 193810399232 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# TRAINING      --> over all batches in the train_loader\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdo_train:\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# VALIDATION    --> avg loss across all batches in valid_loader\u001b[39;00m\n\u001b[0;32m     30\u001b[0m val_loss, val_mre \u001b[38;5;241m=\u001b[39m evaluate(model, loader\u001b[38;5;241m=\u001b[39mvalid_loader)\n",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# ---------- FORWARD PASS -------------------\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m---> 20\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_gene_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     masked_positions \u001b[38;5;241m=\u001b[39m input_values\u001b[38;5;241m.\u001b[39meq(mask_value)  \u001b[38;5;66;03m# the postions to predict\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_mse \u001b[38;5;241m=\u001b[39m criterion(output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlm_output\u001b[39m\u001b[38;5;124m\"\u001b[39m], target_values, masked_positions)\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\Desktop\\240316_bio-llms\\model_bioformer.py:76\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, g, x, z)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     71\u001b[0m     g: Tensor,\n\u001b[0;32m     72\u001b[0m     x: Tensor,\n\u001b[0;32m     73\u001b[0m     z: Optional[Tensor]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     74\u001b[0m     ):\n\u001b[1;32m---> 76\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     output \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# Masked Value Prediction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\Desktop\\240316_bio-llms\\model_bioformer.py:66\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[1;34m(self, g, x, z)\u001b[0m\n\u001b[0;32m     63\u001b[0m total_embs \u001b[38;5;241m=\u001b[39m g \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# output = self.transformer_encoder(total_embs)\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbioformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\Desktop\\240316_bio-llms\\bioformer.py:332\u001b[0m, in \u001b[0;36mBioFormerStack.forward\u001b[1;34m(self, m, z)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, m, z):\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 332\u001b[0m         m, z \u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m, z\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\Desktop\\240316_bio-llms\\bioformer.py:304\u001b[0m, in \u001b[0;36mBioFormerBlock.forward\u001b[1;34m(self, m, z)\u001b[0m\n\u001b[0;32m    302\u001b[0m m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(m, z)\n\u001b[0;32m    303\u001b[0m m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrans(m)\n\u001b[1;32m--> 304\u001b[0m z \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m m, z\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\Desktop\\240316_bio-llms\\bioformer.py:69\u001b[0m, in \u001b[0;36mOuterProductMean.forward\u001b[1;34m(self, m)\u001b[0m\n\u001b[0;32m     65\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_2(ln) \n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m ln\n\u001b[1;32m---> 69\u001b[0m outer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outer\n",
      "File \u001b[1;32mc:\\Users\\giaco\\Desktop\\240316_bio-llms\\bioformer.py:41\u001b[0m, in \u001b[0;36mOuterProductMean._opm\u001b[1;34m(self, a, b)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_opm\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# [*, r, r, c, c]\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     outer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m...ab,...cd->...acbd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# [*, r, r, c * c]\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     outer \u001b[38;5;241m=\u001b[39m outer\u001b[38;5;241m.\u001b[39mreshape(outer\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;66;03m# flatten last two dim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\functional.py:385\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    387\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 193810399232 bytes."
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "if config.wandb:\n",
    "    define_wandb_metrics()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_data_pt, valid_data_pt = prepare_data()\n",
    "    \n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # TRAINING      --> over all batches in the train_loader\n",
    "    if config.do_train:\n",
    "        train(model, loader=train_loader)\n",
    "\n",
    "    # VALIDATION    --> avg loss across all batches in valid_loader\n",
    "    val_loss, val_mre = evaluate(model, loader=valid_loader)\n",
    "    \n",
    "    \n",
    "    # Some epoch-related stats\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print(\"-\" * 89)\n",
    "    print(f\"| end of epoch {epoch:3d} | runtime: {elapsed:5.2f}s | valid/mse {val_loss:5.4f} | valid/mre {val_mre:5.4f}\")\n",
    "    print(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        print(f\"Best model with valid/mse {best_val_loss:5.4f}\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>train/mse</td><td>█▇▆▅▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/nzlp</td><td>█▆▄▃▃▃▃▂▂▃▂▂▃▃▂▂▂▃▃▃▂▂▂▃▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid/mre</td><td>▁</td></tr><tr><td>valid/mse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train/mse</td><td>165.19212</td></tr><tr><td>train/nzlp</td><td>0.00104</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-water-4</strong> at: <a href='https://wandb.ai/giacomo-ciro/BioFormer/runs/ahbxhy4k' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer/runs/ahbxhy4k</a><br/> View project at: <a href='https://wandb.ai/giacomo-ciro/BioFormer' target=\"_blank\">https://wandb.ai/giacomo-ciro/BioFormer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240902_162226-ahbxhy4k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if config.wandb:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioFormer\n",
    "Adaptation of AF2 modules to work with RNA-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tot. params: 465096M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# from bioformer import BioFormerStack\n",
    "\n",
    "# B, r = 64, 18\n",
    "\n",
    "# c_in = 8\n",
    "# c_hidden = 128\n",
    "# no_heads = 4\n",
    "\n",
    "# m = torch.rand((B, r, c_in ))\n",
    "# z = torch.rand((B, r, r, c_in))\n",
    "\n",
    "# model = BioFormerStack(\n",
    "#                             c_m=c_in,\n",
    "#                             c_z=c_in,\n",
    "#                             c_hidden=c_hidden,\n",
    "#                             no_heads=no_heads,\n",
    "#                             no_blocks=3\n",
    "# )\n",
    "\n",
    "# m_out, z_out  = model(m, z)\n",
    "\n",
    "# print(f'''\n",
    "# tot. params: {sum(p.numel() for p in model.parameters())1e6:.2f}M\n",
    "# ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
