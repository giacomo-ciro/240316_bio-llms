{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scGPT\n",
    "Custom implemention of the scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import time\n",
    "import copy\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import set_seed, AttrDict\n",
    "from myTorchtext import Vocab\n",
    "from preprocess import Preprocessor\n",
    "from tokenizer import tokenize_and_pad_batch, retrieve_tfs, random_mask_value\n",
    "from model import TransformerModel\n",
    "from model_bioformer import BioFormerModel\n",
    "from loss import masked_mse_loss, masked_relative_error, criterion_neg_log_bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AttrDict({\n",
    "    \"run_name\": \"\",\n",
    "    \"dataset_name\": \"HYPOXIA_9K\",\n",
    "    \"model\": \"scGPT\",       # BioFormer / scGPT\n",
    "    \"seed\": 5289,\n",
    "    \"d_model\": 32,\n",
    "    \"nhead\": 4,\n",
    "    \"nlayers\": 8,\n",
    "    \"do_pair_bias\": False,\n",
    "    \"do_opm\": False,\n",
    "    \"n_bins\": 51,\n",
    "    \"n_hvg\": 100,\n",
    "    \"include_zero_gene\": False,\n",
    "    \"mask_single_value\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"amp\": True,\n",
    "    \"schedule_ratio\": 0.9,\n",
    "    \"GEPC\": False,   # If Gene Expression Prediction for Cell Modelling objective (MLM from <cls> only) TODO in model.py\n",
    "    \"explicit_zero_prob\": True, # if modelling gene expression also as bern var\n",
    "    \"do_train\": True,\n",
    "    \"log_interval\": 100,\n",
    "    \"wandb\": False,\n",
    "})\n",
    "\n",
    "if config.seed:\n",
    "    set_seed(config.seed)\n",
    "\n",
    "if config.wandb:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(\n",
    "        project='BioFormer',\n",
    "        config = config,\n",
    "        name = config.run_name if config.run_name else None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_value = -1 # in the value vector corresponding to msk token (!= msk token index in vocab)\n",
    "pad_value = -2  # in the value vector corresponding to pad token (!= pad token index in vocab)\n",
    "n_input_bins = config.n_bins\n",
    "include_zero_gene = config.include_zero_gene\n",
    "n_hvg = config.n_hvg\n",
    "max_seq_len = n_hvg + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPOXIA_9K\n",
      "AnnData object with n_obs × n_vars = 9234 × 19046\n",
      "    obs: 'nCount_RNA', 'nFeature_RNA', 'SampleTags', 'percent.mt', 'HypoxicState', 'TimePoint', 'nCount_SCT', 'nFeature_SCT', 'S.Score', 'G2M.Score', 'Phase', 'seurat_clusters', 'SampleTagsShort', 'active_ident'\n",
      "    var: 'variable_gene', 'gene_name'\n",
      "    uns: 'active_ident_colors', 'seurat_clusters_colors'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    layers: 'raw_count'\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "dataset_name = config.dataset_name\n",
    "\n",
    "if dataset_name == 'BREAST_25K':\n",
    "    adata = sc.read_h5ad('./data/breast_25k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'BREAST_12K':\n",
    "    adata = sc.read_h5ad('./data/breast_12k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'DERMAL_100K':\n",
    "    adata = sc.read_h5ad('./data/dermal_100k.h5ad')\n",
    "    adata.var[\"gene_name\"] = adata.var.feature_name.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'HYPOXIA_9K':\n",
    "    adata = sc.read_h5ad('./data/scsHypoxiaTimeSub.h5ad')\n",
    "    adata.X = adata.layers['raw_count']\n",
    "    adata.var['gene_name'] = adata.var.index.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "print(dataset_name)\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering genes by counts ...\n",
      "Normalizing total counts ...\n",
      "Log1p transforming ...\n",
      "Subsetting highly variable genes ...\n",
      "No batch_key is provided, will use all cells for HVG selection.\n",
      "Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# Pre-process adata\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=config.n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab of size: 103 --> 100 genes, 3 special tokens ['<pad>', '<cls>', '<eoc>']\n"
     ]
    }
   ],
   "source": [
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].toarray()\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "train_data, valid_data = train_test_split(all_counts, test_size=0.1, shuffle=True)\n",
    "\n",
    "# Vocab\n",
    "stoi = {s:i for i, s in enumerate(genes + special_tokens)}\n",
    "itos = {i:s for i, s in enumerate(genes + special_tokens)}\n",
    "vocab = Vocab(stoi, itos)\n",
    "vocab.set_default_index(vocab[\"<pad>\"]) # index to return if token not found in vocab\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "print(f'Vocab of size: {len(vocab)} --> {len(genes)} genes, {len(special_tokens)} special tokens {special_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 8310\n",
      "Valid samples: 924\n",
      "Input length: 69\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "print(f\"Train samples: {tokenized_train['genes'].shape[0]}\")\n",
    "print(f\"Valid samples: {tokenized_valid['genes'].shape[0]}\")\n",
    "print(f\"Input length: {tokenized_valid['genes'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(use_condition_labels = False):\n",
    "    \n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "        mask_single_value = config.mask_single_value\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "        mask_single_value = config.mask_single_value\n",
    "    )\n",
    "\n",
    "    print(f\"random masking at epoch {epoch}, ratio of masked values in train: {(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\")\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "    }\n",
    "\n",
    "    # if use_condition_labels:\n",
    "    #     train_data_pt['conditions'] = retrieve_tfs(\n",
    "    #         input_gene_ids_train,\n",
    "    #         input_values_train,     # masked\n",
    "    #         tf = tf                                  \n",
    "    #     )\n",
    "    #     valid_data_pt['conditions'] = retrieve_tfs(\n",
    "    #         input_gene_ids_valid,\n",
    "    #         input_values_valid,      # masked\n",
    "    #         tf = tf                                  \n",
    "    #     )\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: cpu | model: scGPT | d_model: 32 | nhead: 4 | nlayers: 8 | tot. params: 0.11M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ntoken = len(vocab)  # size of vocabulary\n",
    "if config.model == \"scGPT\":\n",
    "    model = TransformerModel(\n",
    "        ntoken=ntoken,\n",
    "        d_model=config.d_model,\n",
    "        nhead=config.nhead,\n",
    "        d_hid=config.d_model*4,\n",
    "        nlayers=config.nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        # pad_value=pad_value,\n",
    "    ) \n",
    "elif config.model == \"BioFormer\":\n",
    "    model = BioFormerModel(\n",
    "        ntoken=ntoken,\n",
    "        d_model=config.d_model,\n",
    "        nhead=config.nhead,\n",
    "        # d_hid=config.d_model,\n",
    "        nlayers=config.nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        # pad_value=pad_value,\n",
    "        do_pair_bias=config.do_pair_bias,\n",
    "        do_opm=config.do_opm,\n",
    "    ) \n",
    "\n",
    "model.to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "if config.wandb:\n",
    "    wandb.config.update({\"Model Parameters\": n_params})\n",
    "\n",
    "print(f'''\n",
    "device: {device} | model: {config.model} | d_model: {config.d_model} | nhead: {config.nhead} | nlayers: {config.nlayers} | tot. params: {n_params/1e6:.2f}M\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_mre = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        \n",
    "        if config.model == \"BioFormer\":\n",
    "            B, r = input_values.shape\n",
    "            z = torch.randn((B, r, r)).to(device)\n",
    "        \n",
    "\n",
    "        # ---------- FORWARD PASS -------------------\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            \n",
    "            if config.model == \"scGPT\":\n",
    "                output_dict = model(input_gene_ids, input_values)\n",
    "            elif config.model == \"BioFormer\":\n",
    "                output_dict = model(input_gene_ids, input_values, z)\n",
    "            \n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = criterion(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "            \n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            \n",
    "            if config.explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mlm_zero_probs\"], target_values, masked_positions)\n",
    "                loss += loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            \n",
    "            if config.GEPC:\n",
    "                loss_gepc = criterion(output_dict[\"mvc_output\"], target_values, masked_positions)\n",
    "                loss += loss_gepc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "            \n",
    "            if config.GEPC and config.explicit_zero_prob:\n",
    "                loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mvc_zero_probs\"], target_values, masked_positions)\n",
    "                loss = loss + loss_gepc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()})\n",
    "\n",
    "        # ---------- BACKWARD PASS ------------------\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()   # training via the aggregated loss\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # -------------------------------------------\n",
    "        \n",
    "        if config.wandb:\n",
    "            wandb.log(metrics_to_log)\n",
    "\n",
    "        # Compute MRE for validation\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "\n",
    "        total_loss += loss.item()                               # sum of all losses\n",
    "        total_mse += loss_mse.item()                            # MSE alone\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0  # MSE from GEPC alone\n",
    "        total_mre += mre.item()                                 # MRE alone\n",
    "        \n",
    "        # Avg of loss across all log_interval batches (i.e., log_interval = 10, avg loss every 10 batches)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_mre = total_mre / log_interval\n",
    "            \n",
    "            print(f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | train/loss {cur_loss:5.2f} | train/mse {cur_mse:5.2f} |\" + (f\"train/gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\") + f\"train/mre {cur_mre:5.2f} |\" )\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_mre = 0\n",
    "            start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_wandb_metrics():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mre = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "\n",
    "            if config.model == \"BioFormer\":\n",
    "                B, r = input_values.shape\n",
    "                z = torch.randn((B, r, r)).to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                \n",
    "                if config.model == \"scGPT\":\n",
    "                    output_dict = model(input_gene_ids, input_values)\n",
    "                elif config.model == \"BioFormer\":\n",
    "                    output_dict = model(input_gene_ids, input_values, z)\n",
    "                \n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_mre += masked_relative_error(output_values, target_values, masked_positions).item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    if config.wandb:\n",
    "        wandb.log({ \n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_mre / total_num,\n",
    "            \"epoch\": epoch\n",
    "            })\n",
    "\n",
    "    return total_loss / total_num, total_mre / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch 1, ratio of masked values in train: 0.1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 100/260 batches | lr 0.0001 | ms/batch 212.72 | train/loss 800.42 | train/mse 800.42 |train/mre 18962.84 |\n",
      "| epoch   1 | 200/260 batches | lr 0.0001 | ms/batch 217.42 | train/loss 744.34 | train/mse 744.34 |train/mre 42051.72 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | runtime: 58.88s | valid/mse 670.9940 | valid/mre 95028.9876\n",
      "-----------------------------------------------------------------------------------------\n",
      "Best model with valid/mse 670.9940\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "if config.wandb:\n",
    "    define_wandb_metrics()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_data_pt, valid_data_pt = prepare_data()\n",
    "    \n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # TRAINING      --> over all batches in the train_loader\n",
    "    if config.do_train:\n",
    "        train(model, loader=train_loader)\n",
    "\n",
    "    # VALIDATION    --> avg loss across all batches in valid_loader\n",
    "    val_loss, val_mre = evaluate(model, loader=valid_loader)\n",
    "    \n",
    "    \n",
    "    # Some epoch-related stats\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print(\"-\" * 89)\n",
    "    print(f\"| end of epoch {epoch:3d} | runtime: {elapsed:5.2f}s | valid/mse {val_loss:5.4f} | valid/mre {val_mre:5.4f}\")\n",
    "    print(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        print(f\"Best model with valid/mse {best_val_loss:5.4f}\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.wandb:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions before filtering: 375,171\n",
      "Interactions after filtering: 127\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('./data/transcriptional_interactions.csv', low_memory=False, index_col = 0)\n",
    "print(f'Interactions before filtering: {df.shape[0]:,}')\n",
    "df = df.loc[df.source_genesymbol.isin(genes)]\n",
    "df = df.loc[df.target_genesymbol.isin(genes)]\n",
    "print(f'Interactions after filtering: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>source_genesymbol</th>\n",
       "      <th>target_genesymbol</th>\n",
       "      <th>is_directed</th>\n",
       "      <th>is_stimulation</th>\n",
       "      <th>is_inhibition</th>\n",
       "      <th>consensus_direction</th>\n",
       "      <th>consensus_stimulation</th>\n",
       "      <th>consensus_inhibition</th>\n",
       "      <th>sources</th>\n",
       "      <th>references</th>\n",
       "      <th>curation_effort</th>\n",
       "      <th>dorothea_level</th>\n",
       "      <th>n_references</th>\n",
       "      <th>n_resources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>P18146</td>\n",
       "      <td>P01584</td>\n",
       "      <td>EGR1</td>\n",
       "      <td>IL1B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ARACNe-GTEx_DoRothEA;CollecTRI;DoRothEA;ENCODE...</td>\n",
       "      <td>CollecTRI:10202038;CollecTRI:12637574;CollecTR...</td>\n",
       "      <td>10</td>\n",
       "      <td>D</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>P01100</td>\n",
       "      <td>P41134</td>\n",
       "      <td>FOS</td>\n",
       "      <td>ID1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DoRothEA;PAZAR_DoRothEA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>P41134</td>\n",
       "      <td>P01100</td>\n",
       "      <td>ID1</td>\n",
       "      <td>FOS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CollecTRI;ExTRI_CollecTRI;NTNU.Curated_CollecTRI</td>\n",
       "      <td>CollecTRI:10319327</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>P01100</td>\n",
       "      <td>P11021</td>\n",
       "      <td>FOS</td>\n",
       "      <td>HSPA5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>CollecTRI;DoRothEA;ENCODE-proximal;ExTRI_Colle...</td>\n",
       "      <td>CollecTRI:10472808;CollecTRI:11719466</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3557</th>\n",
       "      <td>P01100</td>\n",
       "      <td>P03956</td>\n",
       "      <td>FOS</td>\n",
       "      <td>MMP1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>CollecTRI;DoRothEA;ENCODE-proximal;ExTRI_Colle...</td>\n",
       "      <td>CollecTRI:10684971;CollecTRI:10914841;CollecTR...</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332894</th>\n",
       "      <td>P18146</td>\n",
       "      <td>Q13268</td>\n",
       "      <td>EGR1</td>\n",
       "      <td>DHRS2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCODE-distal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332909</th>\n",
       "      <td>P18146</td>\n",
       "      <td>P09914</td>\n",
       "      <td>EGR1</td>\n",
       "      <td>IFIT1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCODE-distal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332950</th>\n",
       "      <td>P18146</td>\n",
       "      <td>P05120</td>\n",
       "      <td>EGR1</td>\n",
       "      <td>SERPINB2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCODE-distal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334749</th>\n",
       "      <td>P01100</td>\n",
       "      <td>P07996</td>\n",
       "      <td>FOS</td>\n",
       "      <td>THBS1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCODE-distal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359937</th>\n",
       "      <td>P01100</td>\n",
       "      <td>Q15327</td>\n",
       "      <td>FOS</td>\n",
       "      <td>ANKRD1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCODE-proximal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source  target source_genesymbol target_genesymbol  is_directed  \\\n",
       "801     P18146  P01584              EGR1              IL1B            1   \n",
       "1137    P01100  P41134               FOS               ID1            1   \n",
       "1138    P41134  P01100               ID1               FOS            1   \n",
       "2241    P01100  P11021               FOS             HSPA5            1   \n",
       "3557    P01100  P03956               FOS              MMP1            1   \n",
       "...        ...     ...               ...               ...          ...   \n",
       "332894  P18146  Q13268              EGR1             DHRS2            1   \n",
       "332909  P18146  P09914              EGR1             IFIT1            1   \n",
       "332950  P18146  P05120              EGR1          SERPINB2            1   \n",
       "334749  P01100  P07996               FOS             THBS1            1   \n",
       "359937  P01100  Q15327               FOS            ANKRD1            1   \n",
       "\n",
       "        is_stimulation  is_inhibition  consensus_direction  \\\n",
       "801                  1              0                    1   \n",
       "1137                 0              0                    0   \n",
       "1138                 0              1                    1   \n",
       "2241                 1              0                    1   \n",
       "3557                 1              0                    1   \n",
       "...                ...            ...                  ...   \n",
       "332894               0              0                    0   \n",
       "332909               0              0                    0   \n",
       "332950               0              0                    0   \n",
       "334749               0              0                    0   \n",
       "359937               0              0                    0   \n",
       "\n",
       "        consensus_stimulation  consensus_inhibition  \\\n",
       "801                         1                     0   \n",
       "1137                        0                     0   \n",
       "1138                        0                     1   \n",
       "2241                        1                     0   \n",
       "3557                        1                     0   \n",
       "...                       ...                   ...   \n",
       "332894                      0                     0   \n",
       "332909                      0                     0   \n",
       "332950                      0                     0   \n",
       "334749                      0                     0   \n",
       "359937                      0                     0   \n",
       "\n",
       "                                                  sources  \\\n",
       "801     ARACNe-GTEx_DoRothEA;CollecTRI;DoRothEA;ENCODE...   \n",
       "1137                              DoRothEA;PAZAR_DoRothEA   \n",
       "1138     CollecTRI;ExTRI_CollecTRI;NTNU.Curated_CollecTRI   \n",
       "2241    CollecTRI;DoRothEA;ENCODE-proximal;ExTRI_Colle...   \n",
       "3557    CollecTRI;DoRothEA;ENCODE-proximal;ExTRI_Colle...   \n",
       "...                                                   ...   \n",
       "332894                                      ENCODE-distal   \n",
       "332909                                      ENCODE-distal   \n",
       "332950                                      ENCODE-distal   \n",
       "334749                                      ENCODE-distal   \n",
       "359937                                    ENCODE-proximal   \n",
       "\n",
       "                                               references  curation_effort  \\\n",
       "801     CollecTRI:10202038;CollecTRI:12637574;CollecTR...               10   \n",
       "1137                                                  NaN                0   \n",
       "1138                                   CollecTRI:10319327                1   \n",
       "2241                CollecTRI:10472808;CollecTRI:11719466                2   \n",
       "3557    CollecTRI:10684971;CollecTRI:10914841;CollecTR...               10   \n",
       "...                                                   ...              ...   \n",
       "332894                                                NaN                0   \n",
       "332909                                                NaN                0   \n",
       "332950                                                NaN                0   \n",
       "334749                                                NaN                0   \n",
       "359937                                                NaN                0   \n",
       "\n",
       "       dorothea_level  n_references  n_resources  \n",
       "801                 D            10            3  \n",
       "1137                D             0            1  \n",
       "1138              NaN             1            1  \n",
       "2241                D             2            3  \n",
       "3557                A             9            4  \n",
       "...               ...           ...          ...  \n",
       "332894            NaN             0            1  \n",
       "332909            NaN             0            1  \n",
       "332950            NaN             0            1  \n",
       "334749            NaN             0            1  \n",
       "359937            NaN             0            1  \n",
       "\n",
       "[127 rows x 16 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given a list of genes, retrieve corresponding TFs\n",
    "def get_z(g, interactions):\n",
    "    \"\"\"\n",
    "    g:\n",
    "        [B, r]:\n",
    "                vector of gene tokens (gene_ids)\n",
    "    interactions:\n",
    "        pandas.DataFrame with columns [source_genesymbol, target_genesymbol, is_stimulation, is_inhibition]\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # import transcriptional_interactions and filter vocabulary\n",
    "    # (maybe a cache to avoid importing and checking vocabulary every time?)\n",
    "    # init (B, r, r) z tensor\n",
    "    # loop through all genes and fill z\n",
    "\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
