{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scGPT\n",
    "Custom implemention of the scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import time\n",
    "import copy\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import set_seed, AttrDict, get_interactions, get_z\n",
    "from myTorchtext import Vocab\n",
    "from preprocess import Preprocessor\n",
    "from tokenizer import tokenize_and_pad_batch, retrieve_tfs, random_mask_value\n",
    "from model import TransformerModel, BioFormerModel\n",
    "# from model_bioformer import BioFormerModel\n",
    "from loss import masked_mse_loss, masked_relative_error, criterion_neg_log_bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AttrDict({\n",
    "    \"run_name\": \"\",\n",
    "    \"dataset_name\": \"HYPOXIA_9K\",\n",
    "    \"model\": \"BioFormer\",       # BioFormer / scGPT\n",
    "    \n",
    "    \"d_model\": 32,\n",
    "    \"nhead\": 4,\n",
    "    \"nlayers\": 8,\n",
    "    \"n_hvg\": 100,\n",
    "    \n",
    "    \"do_pair_bias\": True,\n",
    "    \"do_opm\": True,\n",
    "    \"init_z\": False,\n",
    "\n",
    "    \"do_train\": True,\n",
    "    \"epochs\": 1,\n",
    "\n",
    "    \"wandb\": False,\n",
    "    \"seed\": 5289,\n",
    "    \n",
    "    \"n_bins\": 51,\n",
    "    \"include_zero_gene\": False,\n",
    "    \"mask_single_value\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 32,\n",
    "    \"log_interval\": 100,\n",
    "    \"lr\": 0.0001,\n",
    "    \"amp\": True,\n",
    "    \"schedule_ratio\": 0.9,\n",
    "    \"GEPC\": False,   # If Gene Expression Prediction for Cell Modelling objective (MLM from <cls> only) TODO in model.py\n",
    "    \"explicit_zero_prob\": True, # if modelling gene expression also as bern var\n",
    "\n",
    "})\n",
    "\n",
    "if config.seed:\n",
    "    set_seed(config.seed)\n",
    "\n",
    "if config.wandb:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(\n",
    "        project='BioFormer',\n",
    "        config = config,\n",
    "        name = config.run_name if config.run_name else None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_value = -1 # in the value vector corresponding to msk token (!= msk token index in vocab)\n",
    "pad_value = -2  # in the value vector corresponding to pad token (!= pad token index in vocab)\n",
    "n_input_bins = config.n_bins\n",
    "include_zero_gene = config.include_zero_gene\n",
    "n_hvg = config.n_hvg\n",
    "max_seq_len = n_hvg + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPOXIA_9K\n",
      "AnnData object with n_obs × n_vars = 9234 × 19046\n",
      "    obs: 'nCount_RNA', 'nFeature_RNA', 'SampleTags', 'percent.mt', 'HypoxicState', 'TimePoint', 'nCount_SCT', 'nFeature_SCT', 'S.Score', 'G2M.Score', 'Phase', 'seurat_clusters', 'SampleTagsShort', 'active_ident'\n",
      "    var: 'variable_gene', 'gene_name'\n",
      "    uns: 'active_ident_colors', 'seurat_clusters_colors'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    layers: 'raw_count'\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "path_to_transcriptional_interactions = '../data/transcriptional_interactions.csv'\n",
    "dataset_name = config.dataset_name\n",
    "\n",
    "if dataset_name == 'BREAST_25K':\n",
    "    adata = sc.read_h5ad('../data/breast_25k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'BREAST_12K':\n",
    "    adata = sc.read_h5ad('../data/breast_12k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'DERMAL_100K':\n",
    "    adata = sc.read_h5ad('../data/dermal_100k.h5ad')\n",
    "    adata.var[\"gene_name\"] = adata.var.feature_name.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'HYPOXIA_9K':\n",
    "    adata = sc.read_h5ad('../data/scsHypoxiaTimeSub.h5ad')\n",
    "    adata.X = adata.layers['raw_count']\n",
    "    adata.var['gene_name'] = adata.var.index.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "print(dataset_name)\n",
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering genes by counts ...\n",
      "Normalizing total counts ...\n",
      "Log1p transforming ...\n",
      "Subsetting highly variable genes ...\n",
      "No batch_key is provided, will use all cells for HVG selection.\n",
      "Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# Pre-process adata\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=3,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=config.n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab of size: 103 --> 100 genes, 3 special tokens ['<pad>', '<cls>', '<eoc>']\n"
     ]
    }
   ],
   "source": [
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].toarray()\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "train_data, valid_data = train_test_split(all_counts, test_size=0.1, shuffle=True)\n",
    "\n",
    "# Vocab\n",
    "stoi = {s:i for i, s in enumerate(genes + special_tokens)}\n",
    "itos = {i:s for i, s in enumerate(genes + special_tokens)}\n",
    "vocab = Vocab(stoi, itos)\n",
    "vocab.set_default_index(vocab[\"<pad>\"]) # index to return if token not found in vocab\n",
    "gene_ids = np.array(vocab(genes), dtype=int)\n",
    "print(f'Vocab of size: {len(vocab)} --> {len(genes)} genes, {len(special_tokens)} special tokens {special_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 8310\n",
      "Valid samples: 924\n",
      "Input length: 69\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "print(f\"Train samples: {tokenized_train['genes'].shape[0]}\")\n",
    "print(f\"Valid samples: {tokenized_valid['genes'].shape[0]}\")\n",
    "print(f\"Input length: {tokenized_valid['genes'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(use_condition_labels = False):\n",
    "    \n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "        mask_single_value = config.mask_single_value\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "        mask_single_value = config.mask_single_value\n",
    "    )\n",
    "\n",
    "    print(f\"random masking at epoch {epoch}, ratio of masked values in train: {(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\")\n",
    "\n",
    "    # input_gene_ids_train, input_gene_ids_valid = tokenized_train[\"genes\"], tokenized_valid[\"genes\"]\n",
    "    # input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    # target_values_train, target_values_valid = tokenized_train[\"values\"], tokenized_valid[\"values\"]\n",
    "    B_train, r_train = masked_values_train.shape\n",
    "    B_valid, r_valid = masked_values_valid.shape\n",
    "    if config.init_z:\n",
    "        tf = get_interactions(genes,path_to_transcriptional_interactions)\n",
    "    z_train = get_z(tokenized_train[\"genes\"], tf, vocab.itos) if config.init_z else torch.zeros((B_train, r_train, r_train))    # [B, r, r]\n",
    "    z_valid = get_z(tokenized_valid[\"genes\"], tf, vocab.itos) if config.init_z else torch.zeros((B_valid, r_valid, r_valid))    # [B, r, r]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": tokenized_train[\"genes\"],           # [B, r]\n",
    "        \"values\": masked_values_train,                  # [B, r]\n",
    "        \"target_values\": tokenized_train[\"values\"],     # [B, r]\n",
    "        \"z\": z_train\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": tokenized_valid[\"genes\"],\n",
    "        \"values\": masked_values_valid,\n",
    "        \"target_values\": tokenized_valid[\"values\"],\n",
    "        \"z\": z_valid\n",
    "    }\n",
    "\n",
    "    # if use_condition_labels:\n",
    "    #     train_data_pt['conditions'] = retrieve_tfs(\n",
    "    #         input_gene_ids_train,\n",
    "    #         input_values_train,     # masked\n",
    "    #         tf = tf                                  \n",
    "    #     )\n",
    "    #     valid_data_pt['conditions'] = retrieve_tfs(\n",
    "    #         input_gene_ids_valid,\n",
    "    #         input_values_valid,      # masked\n",
    "    #         tf = tf                                  \n",
    "    #     )\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device: cuda | model: BioFormer | d_model: 32 | nhead: 4 | nlayers: 8 | tot. params: 0.40M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ntoken = len(vocab)  # size of vocabulary\n",
    "if config.model == \"scGPT\":\n",
    "    model = TransformerModel(\n",
    "        ntoken=ntoken,\n",
    "        d_model=config.d_model,\n",
    "        nhead=config.nhead,\n",
    "        d_hid=config.d_model*4,\n",
    "        nlayers=config.nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        # pad_value=pad_value,\n",
    "    ) \n",
    "elif config.model == \"BioFormer\":\n",
    "    model = BioFormerModel(\n",
    "        ntoken=ntoken,\n",
    "        d_model=config.d_model,\n",
    "        nhead=config.nhead,\n",
    "        # d_hid=config.d_model,\n",
    "        nlayers=config.nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        # pad_value=pad_value,\n",
    "        do_pair_bias=config.do_pair_bias,\n",
    "        do_opm=config.do_opm,\n",
    "    ) \n",
    "\n",
    "model.to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "if config.wandb:\n",
    "    wandb.config.update({\"Model Parameters\": n_params})\n",
    "\n",
    "print(f'''\n",
    "device: {device} | model: {config.model} | d_model: {config.d_model} | nhead: {config.nhead} | nlayers: {config.nlayers} | tot. params: {n_params/1e6:.2f}M\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_mre = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        \n",
    "        if config.model == \"BioFormer\":\n",
    "            z = batch_data['z'].to(device)\n",
    "            # B, r = input_values.shape\n",
    "            # z = torch.randn((B, r, r)).to(device)\n",
    "        \n",
    "\n",
    "        # ---------- FORWARD PASS -------------------\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            \n",
    "            if config.model == \"scGPT\":\n",
    "                output_dict = model(input_gene_ids, input_values)\n",
    "            elif config.model == \"BioFormer\":\n",
    "                output_dict = model(input_gene_ids, input_values, z)\n",
    "            \n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = criterion(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "            \n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            \n",
    "            if config.explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mlm_zero_probs\"], target_values, masked_positions)\n",
    "                loss += loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            \n",
    "            if config.GEPC:\n",
    "                loss_gepc = criterion(output_dict[\"mvc_output\"], target_values, masked_positions)\n",
    "                loss += loss_gepc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "            \n",
    "            if config.GEPC and config.explicit_zero_prob:\n",
    "                loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mvc_zero_probs\"], target_values, masked_positions)\n",
    "                loss = loss + loss_gepc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()})\n",
    "\n",
    "        # ---------- BACKWARD PASS ------------------\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()   # training via the aggregated loss\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # -------------------------------------------\n",
    "        \n",
    "        if config.wandb:\n",
    "            wandb.log(metrics_to_log)\n",
    "\n",
    "        # Compute MRE for validation\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "\n",
    "        total_loss += loss.item()                               # sum of all losses\n",
    "        total_mse += loss_mse.item()                            # MSE alone\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0  # MSE from GEPC alone\n",
    "        total_mre += mre.item()                                 # MRE alone\n",
    "        \n",
    "        # Avg of loss across all log_interval batches (i.e., log_interval = 10, avg loss every 10 batches)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_mre = total_mre / log_interval\n",
    "            \n",
    "            print(f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | train/loss {cur_loss:5.2f} | train/mse {cur_mse:5.2f} |\" + (f\"train/gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\") + f\"train/mre {cur_mre:5.2f} |\" )\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_mre = 0\n",
    "            start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_wandb_metrics():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mre = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "\n",
    "            if config.model == \"BioFormer\":\n",
    "                z = batch_data['z'].to(device)\n",
    "                # B, r = input_values.shape\n",
    "                # z = torch.randn((B, r, r)).to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                \n",
    "                if config.model == \"scGPT\":\n",
    "                    output_dict = model(input_gene_ids, input_values)\n",
    "                elif config.model == \"BioFormer\":\n",
    "                    output_dict = model(input_gene_ids, input_values, z)\n",
    "                \n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_mre += masked_relative_error(output_values, target_values, masked_positions).item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    if config.wandb:\n",
    "        wandb.log({ \n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_mre / total_num,\n",
    "            \"epoch\": epoch\n",
    "            })\n",
    "\n",
    "    return total_loss / total_num, total_mre / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch 1, ratio of masked values in train: 0.1383\n",
      "| epoch   1 | 100/260 batches | lr 0.0001 | ms/batch 341.76 | train/loss 766.65 | train/mse 766.65 |train/mre 37954.56 |\n",
      "| epoch   1 | 200/260 batches | lr 0.0001 | ms/batch 339.47 | train/loss 338.91 | train/mse 338.91 |train/mre 372851.27 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | runtime: 91.49s | valid/mse 218.1053 | valid/mre 669170.4669\n",
      "-----------------------------------------------------------------------------------------\n",
      "Best model with valid/mse 218.1053\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "if config.wandb:\n",
    "    define_wandb_metrics()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_data_pt, valid_data_pt = prepare_data()\n",
    "    \n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # TRAINING      --> over all batches in the train_loader\n",
    "    if config.do_train:\n",
    "        train(model, loader=train_loader)\n",
    "\n",
    "    # VALIDATION    --> avg loss across all batches in valid_loader\n",
    "    val_loss, val_mre = evaluate(model, loader=valid_loader)\n",
    "    \n",
    "    \n",
    "    # Some epoch-related stats\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print(\"-\" * 89)\n",
    "    print(f\"| end of epoch {epoch:3d} | runtime: {elapsed:5.2f}s | valid/mse {val_loss:5.4f} | valid/mre {val_mre:5.4f}\")\n",
    "    print(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        print(f\"Best model with valid/mse {best_val_loss:5.4f}\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.wandb:\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
