{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import time\n",
    "import copy\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "from utils import set_seed, AttrDict\n",
    "from vocab import Vocab\n",
    "from preprocess import Preprocessor, get_interactions, get_z\n",
    "from tokenizer import Tokenizer, random_mask_value\n",
    "from model import TransformerModel, BioFormerModel\n",
    "from loss import masked_mse_loss, masked_relative_error, criterion_neg_log_bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mthreshold_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rel'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcooldown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmin_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-08\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Reduce learning rate when a metric has stopped improving.\n",
      "Models often benefit from reducing the learning rate by a factor\n",
      "of 2-10 once learning stagnates. This scheduler reads a metrics\n",
      "quantity and if no improvement is seen for a 'patience' number\n",
      "of epochs, the learning rate is reduced.\n",
      "\n",
      "Args:\n",
      "    optimizer (Optimizer): Wrapped optimizer.\n",
      "    mode (str): One of `min`, `max`. In `min` mode, lr will\n",
      "        be reduced when the quantity monitored has stopped\n",
      "        decreasing; in `max` mode it will be reduced when the\n",
      "        quantity monitored has stopped increasing. Default: 'min'.\n",
      "    factor (float): Factor by which the learning rate will be\n",
      "        reduced. new_lr = lr * factor. Default: 0.1.\n",
      "    patience (int): The number of allowed epochs with no improvement after\n",
      "        which the learning rate will be reduced.\n",
      "        For example, consider the case of having no patience (`patience = 0`).\n",
      "        In the first epoch, a baseline is established and is always considered good as there's no previous baseline.\n",
      "        In the second epoch, if the performance is worse than the baseline,\n",
      "        we have what is considered an intolerable epoch.\n",
      "        Since the count of intolerable epochs (1) is greater than the patience level (0),\n",
      "        the learning rate is reduced at the end of this epoch.\n",
      "        From the third epoch onwards, the learning rate continues to be reduced at the end of each epoch\n",
      "        if the performance is worse than the baseline. If the performance improves or remains the same,\n",
      "        the learning rate is not adjusted.\n",
      "        Default: 10.\n",
      "    threshold (float): Threshold for measuring the new optimum,\n",
      "        to only focus on significant changes. Default: 1e-4.\n",
      "    threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n",
      "        dynamic_threshold = best * ( 1 + threshold ) in 'max'\n",
      "        mode or best * ( 1 - threshold ) in `min` mode.\n",
      "        In `abs` mode, dynamic_threshold = best + threshold in\n",
      "        `max` mode or best - threshold in `min` mode. Default: 'rel'.\n",
      "    cooldown (int): Number of epochs to wait before resuming\n",
      "        normal operation after lr has been reduced. Default: 0.\n",
      "    min_lr (float or list): A scalar or a list of scalars. A\n",
      "        lower bound on the learning rate of all param groups\n",
      "        or each group respectively. Default: 0.\n",
      "    eps (float): Minimal decay applied to lr. If the difference\n",
      "        between new and old lr is smaller than eps, the update is\n",
      "        ignored. Default: 1e-8.\n",
      "    verbose (bool): If ``True``, prints a message to stdout for\n",
      "        each update. Default: ``False``.\n",
      "\n",
      "        .. deprecated:: 2.2\n",
      "            ``verbose`` is deprecated. Please use ``get_last_lr()`` to access the\n",
      "            learning rate.\n",
      "\n",
      "Example:\n",
      "    >>> # xdoctest: +SKIP\n",
      "    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "    >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
      "    >>> for epoch in range(10):\n",
      "    >>>     train(...)\n",
      "    >>>     val_loss = validate(...)\n",
      "    >>>     # Note that step should be called after validate()\n",
      "    >>>     scheduler.step(val_loss)\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\giaco\\miniconda3\\envs\\main\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "torch.optim.lr_scheduler.ReduceLROnPlateau?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run_name': 'test-autorun', 'dataset_name': 'HYPOXIA_9K', 'model': 'scGPT', 'd_model': 32, 'nhead': 4, 'nlayers': 8, 'n_hvg': 50, 'do_pair_bias': True, 'do_opm': True, 'd_z': 32, 'd_opm': 8, 'init_z': False, 'do_train': True, 'epochs': 3, 'batch_size': 16, 'wandb': False, 'seed': 5289, 'n_bins': 51, 'include_zero_gene': False, 'explicit_zero_prob': True, 'log_interval': 100, 'lr': 0.0001, 'amp': True, 'schedule_ratio': 0.9, 'save_model': False}\n",
      "HYPOXIA_9K\n",
      "AnnData object with n_obs × n_vars = 9234 × 19046\n",
      "    obs: 'nCount_RNA', 'nFeature_RNA', 'SampleTags', 'percent.mt', 'HypoxicState', 'TimePoint', 'nCount_SCT', 'nFeature_SCT', 'S.Score', 'G2M.Score', 'Phase', 'seurat_clusters', 'SampleTagsShort', 'active_ident'\n",
      "    var: 'variable_gene', 'gene_name'\n",
      "    uns: 'active_ident_colors', 'seurat_clusters_colors'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    layers: 'raw_count'\n",
      "Filtering genes by counts ...\n",
      "Normalizing total counts ...\n",
      "Log1p transforming ...\n",
      "Subsetting highly variable genes ...\n",
      "No batch_key is provided, will use all cells for HVG selection.\n",
      "Binning data ...\n",
      "Init vocab of size 52 with 50 unique genes...\n",
      "CLS in vocab: 51\n",
      "Tot samples: 9234\n",
      "Input length: 41\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import time\n",
    "import copy\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "from utils import set_seed, AttrDict\n",
    "from vocab import Vocab\n",
    "from preprocess import Preprocessor, get_interactions, get_z\n",
    "from tokenizer import Tokenizer, random_mask_value\n",
    "from model import TransformerModel, BioFormerModel\n",
    "from loss import masked_mse_loss, masked_relative_error, criterion_neg_log_bernoulli\n",
    "\n",
    "config = AttrDict(json.load(open('config.json')))\n",
    "print(config)\n",
    "\n",
    "if config.seed:\n",
    "    set_seed(config.seed)\n",
    "\n",
    "if config.wandb:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    run = wandb.init(\n",
    "        project='BioFormer',\n",
    "        config = config,\n",
    "        name = config.run_name if config.run_name else None\n",
    "    )\n",
    "\n",
    "# Pre-processing\n",
    "# pad_token = \"<pad>\"\n",
    "# special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "# mask_value = -1 # in the value vector corresponding to msk token (!= msk token index in vocab)\n",
    "# pad_value = -2  # in the value vector corresponding to pad token (!= pad token index in vocab)\n",
    "\n",
    "# Import data\n",
    "path_to_transcriptional_interactions = '../data/transcriptional_interactions.csv'\n",
    "dataset_name = config.dataset_name\n",
    "\n",
    "if dataset_name == 'BREAST_25K':\n",
    "    adata = sc.read_h5ad('../data/breast_25k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'BREAST_12K':\n",
    "    adata = sc.read_h5ad('../data/breast_12k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'DERMAL_100K':\n",
    "    adata = sc.read_h5ad('../data/dermal_100k.h5ad')\n",
    "    adata.var[\"gene_name\"] = adata.var.feature_name.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'HYPOXIA_9K':\n",
    "    adata = sc.read_h5ad('../data/scsHypoxiaTimeSub.h5ad')\n",
    "    adata.X = adata.layers['raw_count']\n",
    "    adata.var['gene_name'] = adata.var.index.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "print(dataset_name)\n",
    "print(adata)\n",
    "\n",
    "# Pre-process RNA-seq data\n",
    "preprocessor = Preprocessor(use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "                            filter_gene_by_counts=3,  # step 1\n",
    "                            filter_cell_by_counts=False,  # step 2\n",
    "                            normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "                            result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "                            log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "                            result_log1p_key=\"X_log1p\",\n",
    "                            subset_hvg=config.n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "                            hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "                            binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "                            result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    "                            )\n",
    "preprocessor(adata, batch_key=None)\n",
    "\n",
    "# Vocab\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "vocab = Vocab(genes)\n",
    "vocab.set_default_index(vocab[\"<pad>\"]) # index to return if token not found in vocab\n",
    "print(f'Init vocab of size {len(vocab)} with {config.n_hvg} unique genes...')\n",
    "print(f'CLS in vocab: {vocab.stoi['<cls>']}')\n",
    "\n",
    "# Tokenize & Pad\n",
    "tokenizer = Tokenizer(vocab = vocab,\n",
    "                      append_cls = True,\n",
    "                      cls_token = \"<cls>\",\n",
    "                      pad_token = \"<pad>\",\n",
    "                      pad_value = -2,\n",
    "                      include_zero_gene= config.include_zero_gene, \n",
    "                      )\n",
    "tokenized = tokenizer.tokenize_and_pad_batch(adata.layers[\"X_binned\"].toarray() if issparse(adata.layers[\"X_binned\"]) else adata.layers[\"X_binned\"],\n",
    "                                             np.array(vocab(genes), dtype=int),\n",
    "                                             max_len=config.n_hvg + 1,\n",
    "                                             )\n",
    "print(f\"Tot samples: {tokenized['genes'].shape[0]}\")\n",
    "print(f\"Input length: {tokenized['genes'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 35., -1., 34., 43., 40., 24., 11., 50., 13., 30.,  6., 18., -1.,\n",
       "         26., 11.,  5., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., -1., 11.,  9., 12.,  4., 35., 20., 30., 17., 41., 14., 37., -1.,\n",
       "         47., 23., 43., 50., 12., 20., 33., 13., 30., 12., -1., 39., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 36., 19., 22., 13., 30., -1., 30., 40., 50., -1., 14., 36., 21.,\n",
       "         45.,  2., -1., 11., 26., 43.,  7., 39.,  9., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 46.,  6., 33., -1., 22., 21., 25., 50., 27., 40., 36., -1.,  3.,\n",
       "          9.,  3., 30., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 20., 11., 32., 11.,  1., 15.,  4., 43., 24., 39., 50., -1., 46.,\n",
       "         31., -1., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 14.,  8., -1., 42.,  8., 16., 28., 10., 34., 24., 36., 48., 46.,\n",
       "         38., 13., 18., 26., -1., -1., 21.,  1., 31., 21., 32., 40., 44., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., -1.,  7., 45., 37., 17., 50., 16., 28.,  2.,  9., 29.,  4., 41.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 25., -1., 15., 30., 23., -1., 43., 29.,  4., 36., 46.,  2., 11.,\n",
       "         18.,  5., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0.,  5., -1., 36.,  5., -1., 21., 33., 28., 11., 40., 46., 50., 17.,\n",
       "          5., 43., 30., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 45.,  2., 16., 50., 47., 30., 32., 42.,  8.,  6., -1., 12., 11.,\n",
       "         27., 40., -1., 22.,  5., 21., 22., -1., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 25., 23., -1., 41.,  4., 25., 10., 10., -1., -1., 39., 27.,  2.,\n",
       "         37., 50., 25., 15.,  5., 32.,  2., 26., 35., 45., 47., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 10., 32., 15., 28., 18., 11., 50., -1., 23.,  8., 32., 43.,  1.,\n",
       "         46.,  6., -1., 38., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 45., 33., 17.,  9., 37.,  1., 50., 30., 15., -1., 21., 41., 25.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 24., 21., 39.,  3., -1., 15.,  5., 31., 43., -1., 11., 15., 18.,\n",
       "         28., -1., 50., 25., 47.,  4., 29., 21., 41., 35., 45.,  1., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0.,  1.,  5.,  4.,  5., 50., 30., -1., 36., 39., 22.,  8., 27., 47.,\n",
       "         -1., 26., 20., 41., 16., 44., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.],\n",
       "        [ 0., 32., 25.,  1., 14., -1., 17., 22., 31., 50.,  7.,  2., 18., 46.,\n",
       "         43., -1., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
       "         -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "batch['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlm_output': tensor([[ 6.9914e-02,  1.0955e-01,  1.2178e-01,  4.6768e-02,  1.5070e-01,\n",
       "           1.5231e-01,  7.3118e-02,  3.9406e-02,  4.4318e-03,  5.5104e-02,\n",
       "           5.4776e-02,  6.8119e-02,  6.7590e-02,  6.9702e-02,  2.9926e-02,\n",
       "           3.3949e-02,  1.9985e-01,  1.5031e-01,  1.5031e-01,  1.5031e-01,\n",
       "           1.5031e-01,  1.5031e-01,  1.5031e-01,  1.5031e-01,  1.5031e-01,\n",
       "           1.5031e-01,  1.5031e-01,  1.5031e-01,  1.5031e-01,  1.5031e-01,\n",
       "           1.5031e-01,  1.5031e-01,  1.5031e-01,  1.5031e-01,  1.5031e-01,\n",
       "           1.5031e-01,  1.5031e-01,  1.5031e-01,  1.5031e-01,  1.5031e-01,\n",
       "           1.5031e-01],\n",
       "         [ 8.4718e-02,  9.5660e-02,  9.8267e-02,  4.8800e-02,  1.2695e-01,\n",
       "           9.5691e-02,  1.2213e-01,  1.0694e-01,  1.0744e-01,  3.5587e-02,\n",
       "           2.0089e-02,  3.8034e-02,  3.8196e-02,  1.5177e-01,  1.1638e-01,\n",
       "          -3.1438e-03,  6.9284e-02,  9.0792e-03,  3.6398e-02,  1.3420e-01,\n",
       "           1.7233e-02,  2.0623e-02,  6.5182e-02,  3.6120e-02,  1.2046e-01,\n",
       "           3.5670e-03,  1.0749e-01,  1.0749e-01,  1.0749e-01,  1.0749e-01,\n",
       "           1.0749e-01,  1.0749e-01,  1.0749e-01,  1.0749e-01,  1.0749e-01,\n",
       "           1.0749e-01,  1.0749e-01,  1.0749e-01,  1.0749e-01,  1.0749e-01,\n",
       "           1.0749e-01],\n",
       "         [ 6.7754e-02,  3.0818e-02,  1.0527e-01,  1.4645e-01,  1.4905e-01,\n",
       "           1.0383e-01,  7.6511e-02,  1.0296e-01,  4.2436e-02,  2.7866e-02,\n",
       "           1.5633e-01,  1.3044e-01,  1.8619e-03,  5.4637e-02,  6.9157e-02,\n",
       "           2.8875e-02,  5.1756e-02,  5.4430e-02,  3.0084e-02,  4.1745e-02,\n",
       "           5.4847e-02,  1.6336e-01,  2.7505e-02,  1.0676e-01,  1.0676e-01,\n",
       "           1.0676e-01,  1.0676e-01,  1.0676e-01,  1.0676e-01,  1.0676e-01,\n",
       "           1.0676e-01,  1.0676e-01,  1.0676e-01,  1.0676e-01,  1.0676e-01,\n",
       "           1.0676e-01,  1.0676e-01,  1.0676e-01,  1.0676e-01,  1.0676e-01,\n",
       "           1.0676e-01],\n",
       "         [ 8.3857e-02,  1.1168e-01,  6.1429e-02,  1.5813e-01,  2.2279e-01,\n",
       "           1.7612e-01,  7.5327e-02,  7.2658e-02,  2.7020e-02,  2.9081e-02,\n",
       "           5.7987e-02,  3.3815e-02,  9.3996e-02,  1.3002e-01,  4.4613e-02,\n",
       "           7.9412e-02,  1.8830e-01,  1.5286e-01,  1.5286e-01,  1.5286e-01,\n",
       "           1.5286e-01,  1.5286e-01,  1.5286e-01,  1.5286e-01,  1.5286e-01,\n",
       "           1.5286e-01,  1.5286e-01,  1.5286e-01,  1.5286e-01,  1.5286e-01,\n",
       "           1.5286e-01,  1.5286e-01,  1.5286e-01,  1.5286e-01,  1.5286e-01,\n",
       "           1.5286e-01,  1.5286e-01,  1.5286e-01,  1.5286e-01,  1.5286e-01,\n",
       "           1.5286e-01],\n",
       "         [ 8.9791e-02,  6.6035e-02,  7.1404e-02,  8.6416e-02,  8.9504e-02,\n",
       "           7.7445e-02,  1.1974e-01,  6.4994e-02,  2.5298e-02,  8.7462e-02,\n",
       "           6.3784e-02,  5.2528e-02,  5.9086e-02,  5.1393e-02,  5.9915e-02,\n",
       "           1.7530e-01,  1.2562e-01,  1.2562e-01,  1.2562e-01,  1.2562e-01,\n",
       "           1.2562e-01,  1.2562e-01,  1.2562e-01,  1.2562e-01,  1.2562e-01,\n",
       "           1.2562e-01,  1.2562e-01,  1.2562e-01,  1.2562e-01,  1.2562e-01,\n",
       "           1.2562e-01,  1.2562e-01,  1.2562e-01,  1.2562e-01,  1.2562e-01,\n",
       "           1.2562e-01,  1.2562e-01,  1.2562e-01,  1.2562e-01,  1.2562e-01,\n",
       "           1.2562e-01],\n",
       "         [ 1.1945e-01,  5.8086e-02,  1.2604e-01,  1.9946e-01,  1.8064e-01,\n",
       "           1.6501e-01,  1.4336e-01,  7.9967e-02,  4.9923e-02,  6.4135e-02,\n",
       "           3.8051e-02,  6.3471e-02,  5.9588e-02,  1.6456e-01,  1.4996e-01,\n",
       "           1.5374e-02,  6.6156e-02,  9.7224e-02,  3.2608e-02,  1.1400e-01,\n",
       "           6.9149e-02,  7.3797e-02,  3.9738e-02,  2.5186e-02,  1.1745e-01,\n",
       "           1.5756e-01,  3.3580e-02,  1.3208e-01,  1.3208e-01,  1.3208e-01,\n",
       "           1.3208e-01,  1.3208e-01,  1.3208e-01,  1.3208e-01,  1.3208e-01,\n",
       "           1.3208e-01,  1.3208e-01,  1.3208e-01,  1.3208e-01,  1.3208e-01,\n",
       "           1.3208e-01],\n",
       "         [ 9.7726e-02,  1.3133e-01,  1.4829e-01,  1.8663e-01,  5.6664e-02,\n",
       "           7.8245e-02,  4.2862e-02,  7.7899e-02,  7.5211e-02,  1.0471e-01,\n",
       "           6.1586e-02,  9.8836e-02,  3.3252e-02,  2.0186e-01,  1.5861e-01,\n",
       "           1.5861e-01,  1.5861e-01,  1.5861e-01,  1.5861e-01,  1.5861e-01,\n",
       "           1.5861e-01,  1.5861e-01,  1.5861e-01,  1.5861e-01,  1.5861e-01,\n",
       "           1.5861e-01,  1.5861e-01,  1.5861e-01,  1.5861e-01,  1.5861e-01,\n",
       "           1.5861e-01,  1.5861e-01,  1.5861e-01,  1.5861e-01,  1.5861e-01,\n",
       "           1.5861e-01,  1.5861e-01,  1.5861e-01,  1.5861e-01,  1.5861e-01,\n",
       "           1.5861e-01],\n",
       "         [ 8.7126e-02,  5.9453e-02,  1.2686e-01,  1.5359e-01,  1.5669e-01,\n",
       "           7.9196e-02,  1.5305e-01,  7.7570e-02,  2.9615e-02,  1.2960e-01,\n",
       "           6.7681e-02,  3.9517e-02,  4.0379e-02,  3.4604e-02,  8.9882e-02,\n",
       "           2.0423e-01,  1.6168e-01,  1.6168e-01,  1.6168e-01,  1.6168e-01,\n",
       "           1.6168e-01,  1.6168e-01,  1.6168e-01,  1.6168e-01,  1.6168e-01,\n",
       "           1.6168e-01,  1.6168e-01,  1.6168e-01,  1.6168e-01,  1.6168e-01,\n",
       "           1.6168e-01,  1.6168e-01,  1.6168e-01,  1.6168e-01,  1.6168e-01,\n",
       "           1.6168e-01,  1.6168e-01,  1.6168e-01,  1.6168e-01,  1.6168e-01,\n",
       "           1.6168e-01],\n",
       "         [ 7.6327e-02,  4.4699e-02,  2.0792e-01,  9.0430e-02,  4.9127e-02,\n",
       "           1.4706e-01,  5.9644e-02,  3.0032e-02,  1.4300e-01,  1.5755e-02,\n",
       "           6.6745e-02,  5.7915e-02,  2.0798e-02,  2.5187e-02,  2.7228e-02,\n",
       "           8.7562e-02,  1.8834e-01,  1.5981e-01,  1.5981e-01,  1.5981e-01,\n",
       "           1.5981e-01,  1.5981e-01,  1.5981e-01,  1.5981e-01,  1.5981e-01,\n",
       "           1.5981e-01,  1.5981e-01,  1.5981e-01,  1.5981e-01,  1.5981e-01,\n",
       "           1.5981e-01,  1.5981e-01,  1.5981e-01,  1.5981e-01,  1.5981e-01,\n",
       "           1.5981e-01,  1.5981e-01,  1.5981e-01,  1.5981e-01,  1.5981e-01,\n",
       "           1.5981e-01],\n",
       "         [ 1.2124e-01,  8.2458e-02,  1.5876e-01,  2.4052e-01,  2.2202e-01,\n",
       "           1.2148e-01,  1.8907e-01,  8.9131e-02,  7.9880e-02,  4.6422e-02,\n",
       "           8.8467e-02,  1.8714e-01,  1.7642e-02,  4.2732e-02,  1.0905e-01,\n",
       "           2.4142e-02,  8.1874e-02,  5.1936e-02,  3.8860e-02,  1.3603e-01,\n",
       "           9.2388e-02,  1.7151e-01,  1.4737e-01,  1.4737e-01,  1.4737e-01,\n",
       "           1.4737e-01,  1.4737e-01,  1.4737e-01,  1.4737e-01,  1.4737e-01,\n",
       "           1.4737e-01,  1.4737e-01,  1.4737e-01,  1.4737e-01,  1.4737e-01,\n",
       "           1.4737e-01,  1.4737e-01,  1.4737e-01,  1.4737e-01,  1.4737e-01,\n",
       "           1.4737e-01],\n",
       "         [ 9.6165e-02,  2.1365e-02,  7.6508e-02,  1.3902e-01,  1.4749e-01,\n",
       "           1.3543e-01,  9.0850e-02,  1.2820e-01,  3.1438e-02,  1.2272e-01,\n",
       "           2.3080e-02,  1.2891e-01, -6.3293e-05,  1.0090e-01,  7.4265e-02,\n",
       "           1.9159e-03,  3.1923e-02,  5.9054e-02,  1.5817e-01,  1.5855e-02,\n",
       "           3.3765e-02,  7.5292e-02,  2.2143e-02,  1.5808e-01, -1.1962e-02,\n",
       "           1.2172e-01,  1.2172e-01,  1.2172e-01,  1.2172e-01,  1.2172e-01,\n",
       "           1.2172e-01,  1.2172e-01,  1.2172e-01,  1.2172e-01,  1.2172e-01,\n",
       "           1.2172e-01,  1.2172e-01,  1.2172e-01,  1.2172e-01,  1.2172e-01,\n",
       "           1.2172e-01],\n",
       "         [ 7.1138e-02,  3.3198e-02,  1.1448e-01,  1.5562e-01,  8.5098e-02,\n",
       "           1.2932e-01,  5.3274e-02,  2.7360e-02,  1.0814e-01,  9.1905e-02,\n",
       "           1.3991e-01,  1.3076e-01,  7.1143e-02,  4.3842e-02,  1.2599e-03,\n",
       "           2.6274e-02,  1.8433e-01,  1.7499e-01,  1.4645e-01,  1.4645e-01,\n",
       "           1.4645e-01,  1.4645e-01,  1.4645e-01,  1.4645e-01,  1.4645e-01,\n",
       "           1.4645e-01,  1.4645e-01,  1.4645e-01,  1.4645e-01,  1.4645e-01,\n",
       "           1.4645e-01,  1.4645e-01,  1.4645e-01,  1.4645e-01,  1.4645e-01,\n",
       "           1.4645e-01,  1.4645e-01,  1.4645e-01,  1.4645e-01,  1.4645e-01,\n",
       "           1.4645e-01],\n",
       "         [ 8.3986e-02,  1.1839e-01,  1.9555e-01,  9.6227e-02,  9.8345e-02,\n",
       "           1.0262e-01,  1.0319e-01,  4.9989e-02,  6.6090e-02,  4.2584e-02,\n",
       "           1.9607e-01,  4.4934e-02,  1.5239e-01,  2.1587e-01,  1.7934e-01,\n",
       "           1.7934e-01,  1.7934e-01,  1.7934e-01,  1.7934e-01,  1.7934e-01,\n",
       "           1.7934e-01,  1.7934e-01,  1.7934e-01,  1.7934e-01,  1.7934e-01,\n",
       "           1.7934e-01,  1.7934e-01,  1.7934e-01,  1.7934e-01,  1.7934e-01,\n",
       "           1.7934e-01,  1.7934e-01,  1.7934e-01,  1.7934e-01,  1.7934e-01,\n",
       "           1.7934e-01,  1.7934e-01,  1.7934e-01,  1.7934e-01,  1.7934e-01,\n",
       "           1.7934e-01],\n",
       "         [ 1.0481e-01,  1.2904e-01,  1.8520e-01,  1.6989e-01,  3.9606e-02,\n",
       "           8.3790e-02,  6.2429e-02,  1.3718e-01,  5.3679e-02,  5.7011e-02,\n",
       "           3.1704e-02,  7.3726e-02,  4.8200e-02,  1.1197e-01,  1.5426e-01,\n",
       "           2.4520e-02,  2.1110e-02,  9.8875e-02,  3.8460e-02,  1.7230e-01,\n",
       "           4.1642e-02,  2.2976e-02,  8.7448e-02,  6.6858e-02,  1.6712e-01,\n",
       "           3.4817e-02,  1.1575e-01,  1.1575e-01,  1.1575e-01,  1.1575e-01,\n",
       "           1.1575e-01,  1.1575e-01,  1.1575e-01,  1.1575e-01,  1.1575e-01,\n",
       "           1.1575e-01,  1.1575e-01,  1.1575e-01,  1.1575e-01,  1.1575e-01,\n",
       "           1.1575e-01],\n",
       "         [ 6.6747e-02,  3.3584e-02,  7.6699e-02,  9.5025e-02,  5.2669e-02,\n",
       "           2.5087e-02,  1.2255e-01,  1.6077e-01,  4.6689e-02,  6.6131e-02,\n",
       "           4.8103e-03,  3.9267e-02,  9.6640e-02,  2.7466e-03,  4.8417e-02,\n",
       "           5.5102e-04,  2.9537e-02,  2.6287e-02,  2.8523e-02,  1.4199e-01,\n",
       "           1.0903e-01,  1.0903e-01,  1.0903e-01,  1.0903e-01,  1.0903e-01,\n",
       "           1.0903e-01,  1.0903e-01,  1.0903e-01,  1.0903e-01,  1.0903e-01,\n",
       "           1.0903e-01,  1.0903e-01,  1.0903e-01,  1.0903e-01,  1.0903e-01,\n",
       "           1.0903e-01,  1.0903e-01,  1.0903e-01,  1.0903e-01,  1.0903e-01,\n",
       "           1.0903e-01],\n",
       "         [ 8.7036e-02,  9.0143e-02,  1.3260e-01,  1.4583e-01,  1.0106e-01,\n",
       "           6.5722e-02,  8.0327e-02,  1.1458e-01,  6.4678e-02,  2.4560e-02,\n",
       "           6.7598e-02,  9.8529e-02,  8.6674e-02,  6.3796e-02,  4.6822e-02,\n",
       "           1.8612e-01,  1.2880e-01,  1.2880e-01,  1.2880e-01,  1.2880e-01,\n",
       "           1.2880e-01,  1.2880e-01,  1.2880e-01,  1.2880e-01,  1.2880e-01,\n",
       "           1.2880e-01,  1.2880e-01,  1.2880e-01,  1.2880e-01,  1.2880e-01,\n",
       "           1.2880e-01,  1.2880e-01,  1.2880e-01,  1.2880e-01,  1.2880e-01,\n",
       "           1.2880e-01,  1.2880e-01,  1.2880e-01,  1.2880e-01,  1.2880e-01,\n",
       "           1.2880e-01]], grad_fn=<SqueezeBackward1>),\n",
       " 'mlm_zero_probs': tensor([[0.4999, 0.5059, 0.4950, 0.5068, 0.5088, 0.5063, 0.4996, 0.5132, 0.5061,\n",
       "          0.5020, 0.5064, 0.5094, 0.5143, 0.5114, 0.5069, 0.5027, 0.5043, 0.5008,\n",
       "          0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008,\n",
       "          0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008,\n",
       "          0.5008, 0.5008, 0.5008, 0.5008, 0.5008],\n",
       "         [0.5016, 0.5011, 0.5012, 0.5041, 0.5013, 0.5071, 0.5040, 0.5057, 0.4979,\n",
       "          0.5050, 0.5036, 0.5034, 0.5011, 0.5065, 0.5023, 0.5006, 0.5065, 0.5050,\n",
       "          0.5054, 0.5030, 0.5036, 0.5010, 0.5145, 0.5003, 0.5026, 0.5080, 0.5001,\n",
       "          0.5001, 0.5001, 0.5001, 0.5001, 0.5001, 0.5001, 0.5001, 0.5001, 0.5001,\n",
       "          0.5001, 0.5001, 0.5001, 0.5001, 0.5001],\n",
       "         [0.5051, 0.5017, 0.5021, 0.5100, 0.5063, 0.5075, 0.4915, 0.5088, 0.5084,\n",
       "          0.5052, 0.5081, 0.5035, 0.5010, 0.5028, 0.5083, 0.5020, 0.4926, 0.5068,\n",
       "          0.5043, 0.5167, 0.5011, 0.5029, 0.5091, 0.4999, 0.4999, 0.4999, 0.4999,\n",
       "          0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,\n",
       "          0.4999, 0.4999, 0.4999, 0.4999, 0.4999],\n",
       "         [0.4995, 0.5036, 0.5073, 0.5062, 0.5129, 0.5079, 0.5033, 0.4981, 0.5081,\n",
       "          0.5006, 0.5090, 0.5062, 0.5114, 0.5023, 0.5082, 0.5044, 0.5057, 0.4978,\n",
       "          0.4978, 0.4978, 0.4978, 0.4978, 0.4978, 0.4978, 0.4978, 0.4978, 0.4978,\n",
       "          0.4978, 0.4978, 0.4978, 0.4978, 0.4978, 0.4978, 0.4978, 0.4978, 0.4978,\n",
       "          0.4978, 0.4978, 0.4978, 0.4978, 0.4978],\n",
       "         [0.5098, 0.5045, 0.5086, 0.5060, 0.5089, 0.5025, 0.5123, 0.5121, 0.5091,\n",
       "          0.5046, 0.5110, 0.5126, 0.5101, 0.5181, 0.5056, 0.4947, 0.5027, 0.5027,\n",
       "          0.5027, 0.5027, 0.5027, 0.5027, 0.5027, 0.5027, 0.5027, 0.5027, 0.5027,\n",
       "          0.5027, 0.5027, 0.5027, 0.5027, 0.5027, 0.5027, 0.5027, 0.5027, 0.5027,\n",
       "          0.5027, 0.5027, 0.5027, 0.5027, 0.5027],\n",
       "         [0.5002, 0.5005, 0.4976, 0.4946, 0.5030, 0.5032, 0.4972, 0.5089, 0.5074,\n",
       "          0.5030, 0.5061, 0.5005, 0.4989, 0.5062, 0.5023, 0.5012, 0.4986, 0.5069,\n",
       "          0.4936, 0.5036, 0.5031, 0.5038, 0.5026, 0.4979, 0.5195, 0.5028, 0.5059,\n",
       "          0.5011, 0.5011, 0.5011, 0.5011, 0.5011, 0.5011, 0.5011, 0.5011, 0.5011,\n",
       "          0.5011, 0.5011, 0.5011, 0.5011, 0.5011],\n",
       "         [0.5002, 0.4900, 0.5001, 0.5108, 0.5034, 0.4981, 0.5105, 0.5082, 0.5140,\n",
       "          0.5081, 0.4936, 0.5130, 0.5068, 0.5022, 0.4941, 0.4941, 0.4941, 0.4941,\n",
       "          0.4941, 0.4941, 0.4941, 0.4941, 0.4941, 0.4941, 0.4941, 0.4941, 0.4941,\n",
       "          0.4941, 0.4941, 0.4941, 0.4941, 0.4941, 0.4941, 0.4941, 0.4941, 0.4941,\n",
       "          0.4941, 0.4941, 0.4941, 0.4941, 0.4941],\n",
       "         [0.4984, 0.5138, 0.4921, 0.5045, 0.5048, 0.4982, 0.5002, 0.5185, 0.5084,\n",
       "          0.5063, 0.5165, 0.5139, 0.5093, 0.5070, 0.5184, 0.5035, 0.4984, 0.4984,\n",
       "          0.4984, 0.4984, 0.4984, 0.4984, 0.4984, 0.4984, 0.4984, 0.4984, 0.4984,\n",
       "          0.4984, 0.4984, 0.4984, 0.4984, 0.4984, 0.4984, 0.4984, 0.4984, 0.4984,\n",
       "          0.4984, 0.4984, 0.4984, 0.4984, 0.4984],\n",
       "         [0.4979, 0.5123, 0.4931, 0.4989, 0.5125, 0.5000, 0.5174, 0.5078, 0.5044,\n",
       "          0.5041, 0.5061, 0.5150, 0.5137, 0.5093, 0.5045, 0.5189, 0.5029, 0.4967,\n",
       "          0.4967, 0.4967, 0.4967, 0.4967, 0.4967, 0.4967, 0.4967, 0.4967, 0.4967,\n",
       "          0.4967, 0.4967, 0.4967, 0.4967, 0.4967, 0.4967, 0.4967, 0.4967, 0.4967,\n",
       "          0.4967, 0.4967, 0.4967, 0.4967, 0.4967],\n",
       "         [0.4937, 0.5078, 0.5106, 0.5171, 0.5052, 0.4959, 0.5093, 0.5121, 0.5089,\n",
       "          0.5078, 0.4995, 0.4991, 0.5024, 0.4996, 0.5145, 0.5067, 0.5080, 0.5093,\n",
       "          0.5054, 0.5198, 0.5028, 0.4959, 0.4974, 0.4974, 0.4974, 0.4974, 0.4974,\n",
       "          0.4974, 0.4974, 0.4974, 0.4974, 0.4974, 0.4974, 0.4974, 0.4974, 0.4974,\n",
       "          0.4974, 0.4974, 0.4974, 0.4974, 0.4974],\n",
       "         [0.5012, 0.4988, 0.5033, 0.5048, 0.5049, 0.5018, 0.4969, 0.5073, 0.5063,\n",
       "          0.5043, 0.4994, 0.5022, 0.5000, 0.5033, 0.5052, 0.5040, 0.5028, 0.5015,\n",
       "          0.4999, 0.5008, 0.4983, 0.5157, 0.4986, 0.5041, 0.4984, 0.5020, 0.5020,\n",
       "          0.5020, 0.5020, 0.5020, 0.5020, 0.5020, 0.5020, 0.5020, 0.5020, 0.5020,\n",
       "          0.5020, 0.5020, 0.5020, 0.5020, 0.5020],\n",
       "         [0.4990, 0.5116, 0.5046, 0.5050, 0.4998, 0.5116, 0.5152, 0.5070, 0.4997,\n",
       "          0.5159, 0.5042, 0.5006, 0.5149, 0.5063, 0.5113, 0.5028, 0.5101, 0.5061,\n",
       "          0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008,\n",
       "          0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008, 0.5008,\n",
       "          0.5008, 0.5008, 0.5008, 0.5008, 0.5008],\n",
       "         [0.4958, 0.5028, 0.5074, 0.5036, 0.5000, 0.5083, 0.5125, 0.5099, 0.5178,\n",
       "          0.5091, 0.5065, 0.5082, 0.5102, 0.5079, 0.5004, 0.5004, 0.5004, 0.5004,\n",
       "          0.5004, 0.5004, 0.5004, 0.5004, 0.5004, 0.5004, 0.5004, 0.5004, 0.5004,\n",
       "          0.5004, 0.5004, 0.5004, 0.5004, 0.5004, 0.5004, 0.5004, 0.5004, 0.5004,\n",
       "          0.5004, 0.5004, 0.5004, 0.5004, 0.5004],\n",
       "         [0.5003, 0.5000, 0.5102, 0.5039, 0.5029, 0.4923, 0.5073, 0.5095, 0.5081,\n",
       "          0.5050, 0.4960, 0.5023, 0.5008, 0.5048, 0.5022, 0.5039, 0.5031, 0.5074,\n",
       "          0.5063, 0.5013, 0.5038, 0.4986, 0.5177, 0.4998, 0.5017, 0.5061, 0.4999,\n",
       "          0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999,\n",
       "          0.4999, 0.4999, 0.4999, 0.4999, 0.4999],\n",
       "         [0.5049, 0.5049, 0.5079, 0.5019, 0.5047, 0.5039, 0.5056, 0.4979, 0.5020,\n",
       "          0.4975, 0.5050, 0.5082, 0.5052, 0.5025, 0.5120, 0.5083, 0.5046, 0.5192,\n",
       "          0.5024, 0.5070, 0.5031, 0.5031, 0.5031, 0.5031, 0.5031, 0.5031, 0.5031,\n",
       "          0.5031, 0.5031, 0.5031, 0.5031, 0.5031, 0.5031, 0.5031, 0.5031, 0.5031,\n",
       "          0.5031, 0.5031, 0.5031, 0.5031, 0.5031],\n",
       "         [0.5073, 0.5052, 0.5072, 0.5082, 0.5083, 0.4884, 0.5060, 0.5126, 0.5129,\n",
       "          0.5086, 0.5038, 0.5114, 0.5036, 0.5112, 0.5122, 0.4950, 0.5023, 0.5023,\n",
       "          0.5023, 0.5023, 0.5023, 0.5023, 0.5023, 0.5023, 0.5023, 0.5023, 0.5023,\n",
       "          0.5023, 0.5023, 0.5023, 0.5023, 0.5023, 0.5023, 0.5023, 0.5023, 0.5023,\n",
       "          0.5023, 0.5023, 0.5023, 0.5023, 0.5023]], grad_fn=<SigmoidBackward0>)}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, x = batch['gene_ids'], batch['values']\n",
    "model(g, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (emb_g): GeneEncoder(\n",
      "    (embedding): Embedding(52, 32, padding_idx=50)\n",
      "    (enc_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (emb_x): ContinuousValueEncoder(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (linear1): Linear(in_features=1, out_features=32, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ExprDecoder(\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "    )\n",
      "    (zero_logit): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "device: cpu | model: scGPT | d_model: 32 | nhead: 4 | nlayers: 8 | tot. params: 0.11M | model size: 0.44MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random masking at epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 100/520 batches | lr 0.0000 | ms/batch 74.12 | train/loss 802.87 | train/mse 802.87 |train/mre 16825.11 |\n",
      "| epoch   1 | 200/520 batches | lr 0.0000 | ms/batch 86.45 | train/loss 799.72 | train/mse 799.72 |train/mre 17633.63 |\n",
      "| epoch   1 | 300/520 batches | lr 0.0000 | ms/batch 86.03 | train/loss 810.66 | train/mse 810.66 |train/mre 18230.28 |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 153\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscGPT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 153\u001b[0m         output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_gene_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBioFormer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    155\u001b[0m         output_dict \u001b[38;5;241m=\u001b[39m model(input_gene_ids, input_values, z)\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\Desktop\\240316_bio-llms\\bioformer\\model.py:181\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, g, x)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    178\u001b[0m     g: Tensor,\n\u001b[0;32m    179\u001b[0m     x: Tensor,\n\u001b[0;32m    180\u001b[0m     ):\n\u001b[1;32m--> 181\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     output \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# Masked Value Prediction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\Desktop\\240316_bio-llms\\bioformer\\model.py:173\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[1;34m(self, g, x)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_gene_token_embs \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m total_embs \u001b[38;5;241m=\u001b[39m g \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m--> 173\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_embs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:415\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    412\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 415\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    418\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:749\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    747\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 749\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    750\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:757\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    756\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 757\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1266\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1252\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1253\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1264\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1266\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\giaco\\miniconda3\\envs\\main\\Lib\\site-packages\\torch\\nn\\functional.py:5505\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5502\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   5504\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[1;32m-> 5505\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mattn_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   5507\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n\u001b[0;32m   5508\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz, attn_output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config.do_train = True\n",
    "\n",
    "# Instantiate model\n",
    "if config.model == \"scGPT\":\n",
    "    model = TransformerModel(ntoken=len(vocab),\n",
    "                             d_model=config.d_model,\n",
    "                             nhead=config.nhead,\n",
    "                             nlayers=config.nlayers,\n",
    "                             pad_id = vocab.stoi['<pad>'],\n",
    "                             explicit_zero_prob=config.explicit_zero_prob\n",
    "                             ) \n",
    "elif config.model == \"BioFormer\":\n",
    "    model = BioFormerModel(ntoken=len(vocab),\n",
    "                           d_model=config.d_model,\n",
    "                           d_z = config.d_z,\n",
    "                           d_opm = config.d_opm,\n",
    "                           nhead=config.nhead,\n",
    "                           nlayers=config.nlayers,\n",
    "                           do_pair_bias=config.do_pair_bias,\n",
    "                           do_opm=config.do_opm,\n",
    "                           pad_id = vocab.stoi['<pad>'],\n",
    "                           explicit_zero_prob=config.explicit_zero_prob\n",
    "                           ) \n",
    "print(model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Parameters count\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f'''device: {device} | model: {config.model} | d_model: {config.d_model} | nhead: {config.nhead} | nlayers: {config.nlayers} | tot. params: {n_params/1e6:.2f}M | model size: {model_size_bytes/1e6:.2f}MB''')\n",
    "if config.wandb:\n",
    "    wandb.config.update({\"Model Parameters\": n_params})\n",
    "\n",
    "# Max memory required per intermediate step:\n",
    "if config.model == 'BioFormer':\n",
    "        # no. elements in the [B, r, r, c, c] opm intermediate matrix\n",
    "        tmp = config.batch_size * (config.n_hvg + 1) ** 2 * config.d_opm ** 2\n",
    "        \n",
    "        # number of bytes required (using np.float32)\n",
    "        tmp = tmp * 4\n",
    "\n",
    "        print(f'memory required for opm: {tmp/1e6 :,.2f}MB')\n",
    "        \n",
    "\n",
    "# RNA-seq Dataset\n",
    "class SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Create RNA-seq dataset from vocabulary with keys ['gene_ids', 'valeus', 'target_vaules', 'interactions'].\n",
    "    \"\"\"\n",
    "    def __init__(self, data: dict):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "# Mask and get interactions\n",
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    1. Random mask the data\n",
    "    2. Get the interaction matrix z\n",
    "    3. Convert to torch.Dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    masked_values = random_mask_value(tokenized[\"values\"])\n",
    "    print(f\"Random masking at epoch {epoch}...\")\n",
    "\n",
    "    B, r = masked_values.shape\n",
    "    if config.init_z:\n",
    "        tf = get_interactions(genes, path_to_transcriptional_interactions)\n",
    "    interactions = get_z(tokenized[\"genes\"], tf, vocab.itos) if config.init_z else torch.zeros((B, r, r))    # [B, r, r]\n",
    "\n",
    "    data_pt = {\n",
    "        \"gene_ids\": tokenized[\"genes\"],           # [B, r]\n",
    "        \"values\": masked_values,                  # [B, r]\n",
    "        \"target_values\": tokenized[\"values\"],     # [B, r]\n",
    "        \"interactions\": interactions              # [B, r, r]\n",
    "    }\n",
    "\n",
    "    return SeqDataset(data_pt)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# --------------------------- TRAINING LOOP --------------------------------- #\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "criterion = masked_mse_loss\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=config.lr,\n",
    "                              eps=1e-4 if config.amp else 1e-8\n",
    "                              )\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                            1,\n",
    "                                            gamma=config.schedule_ratio\n",
    "                                            )\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "if config.wandb:\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    dataset = prepare_data()\n",
    "\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # -------------------------------- TRAINING ----------------------------------- #\n",
    "    if config.do_train:\n",
    "        model.train()\n",
    "\n",
    "        loader = train_loader\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_mse = 0.0\n",
    "        total_gepc = 0.0\n",
    "        total_mre = 0.0\n",
    "        log_interval = config.log_interval\n",
    "        start_time = time.time()\n",
    "\n",
    "        num_batches = len(loader)\n",
    "        for batch, batch_data in enumerate(loader):\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            \n",
    "            if config.model == \"BioFormer\":\n",
    "                z = batch_data['interactions'].to(device)\n",
    "\n",
    "            # ---------- forward -------------------\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                \n",
    "                if config.model == \"scGPT\":\n",
    "                    output_dict = model(input_gene_ids, input_values)\n",
    "                elif config.model == \"BioFormer\":\n",
    "                    output_dict = model(input_gene_ids, input_values, z)\n",
    "                \n",
    "                masked_positions = input_values.eq(-1)          # default value for the mask position\n",
    "                loss = loss_mse = criterion(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "                \n",
    "                metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "                \n",
    "                if config.explicit_zero_prob:\n",
    "                    loss_zero_log_prob = criterion_neg_log_bernoulli(output_dict[\"mlm_zero_probs\"], target_values, masked_positions)\n",
    "                    loss += loss_zero_log_prob\n",
    "                    metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "                \n",
    "            # -------------- backward ------------------\n",
    "            model.zero_grad()\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            scheduler.step()\n",
    "            \n",
    "            # --------------- logs & stats ---------------------\n",
    "            if config.wandb:\n",
    "                wandb.log(metrics_to_log)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mre = masked_relative_error(output_dict[\"mlm_output\"], target_values, masked_positions)\n",
    "\n",
    "            total_loss += loss.item()                               # sum of all losses\n",
    "            total_mse += loss_mse.item()                            # MSE alone\n",
    "            total_mre += mre.item()                                 # MRE alone\n",
    "            \n",
    "            # For logging purposes, aggregate loss across log_interval batches \n",
    "            if batch % log_interval == 0 and batch > 0:\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "                cur_loss = total_loss / log_interval\n",
    "                cur_mse = total_mse / log_interval\n",
    "                cur_mre = total_mre / log_interval\n",
    "                \n",
    "                print(f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | train/loss {cur_loss:5.2f} | train/mse {cur_mse:5.2f} |\" + f\"train/mre {cur_mre:5.2f} |\" )\n",
    "                \n",
    "                total_loss = 0\n",
    "                total_mse = 0\n",
    "                total_mre = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "    # -------------------------------- VALIDATION ----------------------------------- #\n",
    "    model.eval()\n",
    "    \n",
    "    loader = valid_loader\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_mre = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "\n",
    "            if config.model == \"BioFormer\":\n",
    "                interactions = batch_data['interactions'].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                \n",
    "                if config.model == \"scGPT\":\n",
    "                    output_dict = model(input_gene_ids, input_values)\n",
    "                elif config.model == \"BioFormer\":\n",
    "                    output_dict = model(input_gene_ids, input_values, interactions)\n",
    "                \n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(-1)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_mre += masked_relative_error(output_values, target_values, masked_positions).item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    if config.wandb:\n",
    "        wandb.log({ \n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_mre / total_num,\n",
    "            \"epoch\": epoch\n",
    "            })\n",
    "\n",
    "    val_loss = total_loss / total_num\n",
    "    val_mre = total_mre / total_num\n",
    "    \n",
    "    # -------------------------------- EPOCH-RELATED STATS ----------------------------------- #\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print(\"-\" * 89)\n",
    "    print(f\"| end of epoch {epoch:3d} | runtime: {elapsed:5.2f}s | valid/mse {val_loss:5.4f} | valid/mre {val_mre:5.4f}\")\n",
    "    print(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        print(f\"New best model found at epoch {epoch} with valid/mse {best_val_loss:5.4f}\")\n",
    "# --------------------------------- END OF TRAINING LOOP -------------------------------------- #\n",
    "\n",
    "# --------------------------------- final house-keeping --------------------------------------- #\n",
    "if config.save_model:\n",
    "    if config.save_model[-1] != \"/\":\n",
    "        config.save_model += \"/\"\n",
    "    dir = f\"{config.save_model}/{config.run_name}_{time.time():.0f}.pt\"\n",
    "    torch.save(best_model.state_dict(), dir)\n",
    "\n",
    "if config.wandb:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run_name': 'bioformer-af2', 'dataset_name': 'HYPOXIA_9K', 'model': 'BioFormer', 'd_model': 256, 'nhead': 8, 'nlayers': 48, 'n_hvg': 255, 'do_pair_bias': True, 'do_opm': True, 'd_z': 128, 'd_opm': 32, 'init_z': False, 'do_train': True, 'epochs': 5, 'batch_size': 16, 'wandb': True, 'seed': 5289, 'n_bins': 51, 'include_zero_gene': False, 'explicit_zero_prob': True, 'log_interval': 100, 'lr': 0.01, 'amp': True, 'schedule_ratio': 0.1, 'save_model': '../checkpoints/'}\n",
      "HYPOXIA_9K\n",
      "AnnData object with n_obs × n_vars = 9234 × 19046\n",
      "    obs: 'nCount_RNA', 'nFeature_RNA', 'SampleTags', 'percent.mt', 'HypoxicState', 'TimePoint', 'nCount_SCT', 'nFeature_SCT', 'S.Score', 'G2M.Score', 'Phase', 'seurat_clusters', 'SampleTagsShort', 'active_ident'\n",
      "    var: 'variable_gene', 'gene_name'\n",
      "    uns: 'active_ident_colors', 'seurat_clusters_colors'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    layers: 'raw_count'\n",
      "Filtering genes by counts ...\n",
      "Normalizing total counts ...\n",
      "Log1p transforming ...\n",
      "Subsetting highly variable genes ...\n",
      "No batch_key is provided, will use all cells for HVG selection.\n",
      "Binning data ...\n",
      "Init vocab of size 257 with 255 unique genes...\n",
      "CLS in vocab: 256\n",
      "Tot samples: 9234\n",
      "Input length: 171\n",
      "BioFormerModel(\n",
      "  (emb_g): GeneEncoder(\n",
      "    (embedding): Embedding(257, 256, padding_idx=255)\n",
      "    (enc_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (emb_x): ContinuousValueEncoder(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (linear1): Linear(in_features=1, out_features=256, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (emb_z): ContinuousValueEncoder(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (linear1): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (bioformer): BioFormerStack(\n",
      "    (blocks): ModuleList(\n",
      "      (0-47): 48 x BioFormerBlock(\n",
      "        (attn): RowAttentionWithPairBias(\n",
      "          (layer_norm_m): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (layer_norm_z): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear_z): Linear(in_features=128, out_features=8, bias=False)\n",
      "          (linear_q): Linear(in_features=256, out_features=32, bias=False)\n",
      "          (linear_k): Linear(in_features=256, out_features=32, bias=False)\n",
      "          (linear_v): Linear(in_features=256, out_features=32, bias=False)\n",
      "          (linear_o): Linear(in_features=32, out_features=256, bias=True)\n",
      "        )\n",
      "        (trans): Transition(\n",
      "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (opm): OuterProductMean(\n",
      "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear_1): Linear(in_features=256, out_features=32, bias=True)\n",
      "          (linear_2): Linear(in_features=256, out_features=32, bias=True)\n",
      "          (linear_out): Linear(in_features=1024, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ExprDecoder(\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (zero_logit): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "device: cpu | model: BioFormer | d_model: 256 | nhead: 8 | nlayers: 48 | tot. params: 34.45M | model size: 137.79MB\n",
      "memory required for opm: 4,294.97MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import time\n",
    "import copy\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "from utils import set_seed, AttrDict\n",
    "from vocab import Vocab\n",
    "from preprocess import Preprocessor, get_interactions, get_z\n",
    "from tokenizer import Tokenizer, random_mask_value\n",
    "from model import TransformerModel, BioFormerModel\n",
    "from loss import masked_mse_loss, masked_relative_error, criterion_neg_log_bernoulli\n",
    "\n",
    "PATH = '../checkpoints/bioformer-af2_1730519158'\n",
    "config = AttrDict(json.load(open(PATH + '.json')))\n",
    "print(config)\n",
    "\n",
    "if config.seed:\n",
    "    set_seed(config.seed)\n",
    "\n",
    "# Import data\n",
    "path_to_transcriptional_interactions = '../data/transcriptional_interactions.csv'\n",
    "dataset_name = config.dataset_name\n",
    "\n",
    "if dataset_name == 'BREAST_25K':\n",
    "    adata = sc.read_h5ad('../data/breast_25k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'BREAST_12K':\n",
    "    adata = sc.read_h5ad('../data/breast_12k.h5ad')\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'DERMAL_100K':\n",
    "    adata = sc.read_h5ad('../data/dermal_100k.h5ad')\n",
    "    adata.var[\"gene_name\"] = adata.var.feature_name.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "elif dataset_name == 'HYPOXIA_9K':\n",
    "    adata = sc.read_h5ad('../data/scsHypoxiaTimeSub.h5ad')\n",
    "    adata.X = adata.layers['raw_count']\n",
    "    adata.var['gene_name'] = adata.var.index.tolist()\n",
    "    data_is_raw = True\n",
    "\n",
    "print(dataset_name)\n",
    "print(adata)\n",
    "\n",
    "# Pre-process RNA-seq data\n",
    "preprocessor = Preprocessor(use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "                            filter_gene_by_counts=3,  # step 1\n",
    "                            filter_cell_by_counts=False,  # step 2\n",
    "                            normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "                            result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "                            log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "                            result_log1p_key=\"X_log1p\",\n",
    "                            subset_hvg=config.n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "                            hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "                            binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "                            result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    "                            )\n",
    "preprocessor(adata, batch_key=None)\n",
    "\n",
    "# Vocab\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "vocab = Vocab(genes)\n",
    "vocab.set_default_index(vocab[\"<pad>\"]) # index to return if token not found in vocab\n",
    "print(f'Init vocab of size {len(vocab)} with {config.n_hvg} unique genes...')\n",
    "print(f\"CLS in vocab: {vocab.stoi['<cls>']}\")\n",
    "\n",
    "# Tokenize & Pad\n",
    "tokenizer = Tokenizer(vocab = vocab,\n",
    "                      append_cls = True,\n",
    "                      cls_token = \"<cls>\",\n",
    "                      pad_token = \"<pad>\",\n",
    "                      pad_value = -2,\n",
    "                      include_zero_gene= config.include_zero_gene, \n",
    "                      )\n",
    "tokenized = tokenizer.tokenize_and_pad_batch(adata.layers[\"X_binned\"].toarray() if issparse(adata.layers[\"X_binned\"]) else adata.layers[\"X_binned\"],\n",
    "                                             np.array(vocab(genes), dtype=int),\n",
    "                                             max_len=config.n_hvg + 1,\n",
    "                                             )\n",
    "print(f\"Tot samples: {tokenized['genes'].shape[0]}\")\n",
    "print(f\"Input length: {tokenized['genes'].shape[1]}\")\n",
    "\n",
    "# Instantiate model\n",
    "if config.model == \"scGPT\":\n",
    "    model = TransformerModel(ntoken=len(vocab),\n",
    "                             d_model=config.d_model,\n",
    "                             nhead=config.nhead,\n",
    "                             nlayers=config.nlayers,\n",
    "                             pad_id = vocab.stoi['<pad>'],\n",
    "                             explicit_zero_prob=config.explicit_zero_prob\n",
    "                             ) \n",
    "    model.load_state_dict(torch.load(PATH + '.pt', weights_only=True))\n",
    "\n",
    "elif config.model == \"BioFormer\":\n",
    "    model = BioFormerModel(ntoken=len(vocab),\n",
    "                           d_model=config.d_model,\n",
    "                           d_z = config.d_z,\n",
    "                           d_opm = config.d_opm,\n",
    "                           nhead=config.nhead,\n",
    "                           nlayers=config.nlayers,\n",
    "                           do_pair_bias=config.do_pair_bias,\n",
    "                           do_opm=config.do_opm,\n",
    "                           pad_id = vocab.stoi['<pad>'],\n",
    "                           explicit_zero_prob=config.explicit_zero_prob\n",
    "                           ) \n",
    "    state_dict = torch.load(PATH + '.pt', weights_only=True, map_location=torch.device('cpu'))\n",
    "    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "print(model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Parameters count\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f'''device: {device} | model: {config.model} | d_model: {config.d_model} | nhead: {config.nhead} | nlayers: {config.nlayers} | tot. params: {n_params/1e6:.2f}M | model size: {model_size_bytes/1e6:.2f}MB''')\n",
    "\n",
    "# Max memory required per intermediate step:\n",
    "if config.model == 'BioFormer':\n",
    "        # no. elements in the [B, r, r, c, c] opm intermediate matrix\n",
    "        tmp = config.batch_size * (config.n_hvg + 1) ** 2 * config.d_opm ** 2\n",
    "        \n",
    "        # number of bytes required (using np.float32)\n",
    "        tmp = tmp * 4\n",
    "\n",
    "        print(f'memory required for opm: {tmp/1e6 :,.2f}MB')\n",
    "        \n",
    "\n",
    "# RNA-seq Dataset\n",
    "class SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Create RNA-seq dataset from vocabulary with keys ['gene_ids', 'valeus', 'target_vaules', 'interactions'].\n",
    "    \"\"\"\n",
    "    def __init__(self, data: dict):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "# Mask and get interactions\n",
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    1. Random mask the data\n",
    "    2. Get the interaction matrix z\n",
    "    3. Convert to torch.Dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    masked_values = random_mask_value(tokenized[\"values\"])\n",
    "    # print(f\"Random masking at epoch {epoch}...\")\n",
    "\n",
    "    B, r = masked_values.shape\n",
    "    if config.init_z:\n",
    "        tf = get_interactions(genes, path_to_transcriptional_interactions)\n",
    "    interactions = get_z(tokenized[\"genes\"], tf, vocab.itos) if config.init_z else torch.zeros((B, r, r))    # [B, r, r]\n",
    "\n",
    "    data_pt = {\n",
    "        \"gene_ids\": tokenized[\"genes\"],           # [B, r]\n",
    "        \"values\": masked_values,                  # [B, r]\n",
    "        \"target_values\": tokenized[\"values\"],     # [B, r]\n",
    "        \"interactions\": interactions              # [B, r, r]\n",
    "    }\n",
    "\n",
    "    return SeqDataset(data_pt)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# ------------------------------ TEST MODEL --------------------------------- #\n",
    "# --------------------------------------------------------------------------- #                                         )\n",
    "\n",
    "criterion = masked_mse_loss\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "dataset = prepare_data()\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0.0\n",
    "total_mre = 0.0\n",
    "total_num = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batch_data = next(iter(loader))\n",
    "batch_data = dataset[[0]]\n",
    "\n",
    "input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "input_values = batch_data[\"values\"].to(device)\n",
    "target_values = batch_data[\"target_values\"].to(device)\n",
    "if config.model == \"BioFormer\":\n",
    "            interactions = batch_data['interactions'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input_gene_ids, input_values, interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for i, batch_data in enumerate(loader):\n",
    "        \n",
    "#         print(f'Batch no. {i}')\n",
    "\n",
    "#         input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "#         input_values = batch_data[\"values\"].to(device)\n",
    "#         target_values = batch_data[\"target_values\"].to(device)\n",
    "\n",
    "#         if config.model == \"BioFormer\":\n",
    "#             interactions = batch_data['interactions'].to(device)\n",
    "\n",
    "#         with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            \n",
    "#             if config.model == \"scGPT\":\n",
    "#                 output_dict = model(input_gene_ids, input_values)\n",
    "#             elif config.model == \"BioFormer\":\n",
    "#                 output_dict = model(input_gene_ids, input_values, interactions)\n",
    "            \n",
    "#             output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "#             masked_positions = input_values.eq(-1)\n",
    "#             loss = criterion(output_values, target_values, masked_positions)\n",
    "\n",
    "#         total_loss += loss.item() * len(input_gene_ids)\n",
    "#         total_mre += masked_relative_error(output_values, target_values, masked_positions).item() * len(input_gene_ids)\n",
    "#         total_num += len(input_gene_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
